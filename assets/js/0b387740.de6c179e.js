"use strict";(self.webpackChunkml_notebook=self.webpackChunkml_notebook||[]).push([[8467],{69676:function(e){e.exports=JSON.parse('{"pluginId":"docs","version":"current","label":"Next","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"\u9996\u9875","href":"/"},{"type":"category","label":"\u7b2c\u96f6\u7ae0\uff1a\u5728\u5f00\u59cb\u4e4b\u524d","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u6bc1\u706d\u7f51\u53cb\u7684\u4eba\u5de5\u667a\u80fd","href":"/ch0/[1]ai-that-destroying-netizens"},{"type":"link","label":"\u9009\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6","href":"/ch0/[2]nn-and-frameworks"},{"type":"link","label":"\u914d\u7f6eGPU","href":"/ch0/[3]configure-gpu"},{"type":"link","label":"\u4f7f\u7528conda\u521b\u5efa\u4e00\u4e2a\u73af\u5883","href":"/ch0/[4]create-new-environment-using-conda"}]},{"type":"category","label":"\u7b2c\u4e00\u7ae0\u4e0a\uff1aHelloWorld","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u6570\u636e\u64cd\u4f5c","href":"/ch1p1/[1]operate-on-data"},{"type":"link","label":"\u81ea\u52a8\u6c42\u68af\u5ea6","href":"/ch1p1/[2]automatic-gradient"},{"type":"link","label":"\u7ebf\u6027\u56de\u5f52","href":"/ch1p1/[3]linear-regression"},{"type":"link","label":"\u7ebf\u6027\u56de\u5f52\u4ee3\u7801\u5b9e\u73b0","href":"/ch1p1/[4]linear-regression-code"},{"type":"link","label":"softmax\u56de\u5f52","href":"/ch1p1/[5]softmax-regression"},{"type":"link","label":"softmax\u56de\u5f52\u7684\u4ee3\u7801\u5b9e\u73b0","href":"/ch1p1/[6]softmax-regression-code"}]},{"type":"category","label":"\u7b2c\u4e00\u7ae0\u4e0b\uff1a\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u591a\u5c42\u611f\u77e5\u673a","href":"/ch1p2/[1]multilayer-perceptron"},{"type":"link","label":"\u591a\u5c42\u611f\u77e5\u673a\u7684\u4ee3\u7801\u5b9e\u73b0","href":"/ch1p2/[2]multilayer-perceptron-code"},{"type":"link","label":"\u6fc0\u6d3b\u51fd\u6570","href":"/ch1p2/[3]activation-functions"},{"type":"link","label":"\u6a21\u578b\u9009\u62e9\u3001\u6b20\u62df\u5408\u548c\u8fc7\u62df\u5408","href":"/ch1p2/[4]underfit-and-overfit"},{"type":"link","label":"\u6743\u91cd\u8870\u51cf","href":"/ch1p2/[5]weight-decay"},{"type":"link","label":"\u4e22\u5f03\u6cd5","href":"/ch1p2/[6]dropout"},{"type":"link","label":"\u6b63\u5411\u4f20\u64ad\u3001\u53cd\u5411\u4f20\u64ad\u548c\u8ba1\u7b97\u56fe","href":"/ch1p2/[7]forward-and-backprop"},{"type":"link","label":"\u6570\u503c\u7a33\u5b9a\u6027\u3001\u6a21\u578b\u521d\u59cb\u5316\u3001\u978d\u70b9\u6536\u655b\u95ee\u9898","href":"/ch1p2/[8]numerical-stability-and-initializing"}]},{"type":"category","label":"\u7b2c\u4e8c\u7ae0\u4e0a\uff1a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","href":"/ch2p1/[1]convolutional-nn-and-ops"},{"type":"link","label":"\u5377\u79ef\u548c\u4ee3\u7801\u5b9e\u73b0","href":"/ch2p1/[2]try-conv-with-code"},{"type":"link","label":"\u586b\u5145\u548c\u6b65\u5e45","href":"/ch2p1/[3]padding-and-strides"},{"type":"link","label":"\u5377\u79ef\u4e2d\u7684\u901a\u9053","href":"/ch2p1/[4]about-channels-in-conv"},{"type":"link","label":"\u6c60\u5316\u5c42","href":"/ch2p1/[5]about-pooling"},{"type":"link","label":"\u6807\u51c6\u5316\u5904\u7406","href":"/ch2p1/[6]batch-normolization"},{"type":"link","label":"\u80e1\u8d1d\u5c14\u548c\u5a01\u585e\u5c14\u7684\u732b","href":"/ch2p1/[7]CNN-and-visual-cortex"}]},{"type":"category","label":"\u7b2c\u4e8c\u7ae0\u4e0b\uff1a\u7ecf\u5178\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"LeNet\uff1a\u521d\u8bd5\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","href":"/ch2p2/[1]LeNet"},{"type":"link","label":"LeNet\u4ee3\u7801\u5b9e\u73b0","href":"/ch2p2/[2]LeNet-code"},{"type":"link","label":"\u65b0\u73a9\u5177\uff1aKeras API","href":"/ch2p2/[3]write-code-with-keras"},{"type":"link","label":"AlexNet\uff1a\u66f4\u6df1\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","href":"/ch2p2/[4]AlexNet"},{"type":"link","label":"AlexNet\u4ee3\u7801\u5b9e\u73b0","href":"/ch2p2/[5]AlexNet-code"},{"type":"link","label":"\u6807\u51c6\u5316\u5c42\u548c\u6fc0\u6d3b\u5c42\u7684\u987a\u5e8f\u95ee\u9898","href":"/ch2p2/[6]the-sequence-order-between-bn-and-activations"},{"type":"link","label":"VGG\uff1a\u53ef\u590d\u7528\u7684\u7f51\u7edc\u5757","href":"/ch2p2/[7]VGGNet"},{"type":"link","label":"VGG\u7684\u4ee3\u7801\u5b9e\u73b0","href":"/ch2p2/[8]VGGNet-code"},{"type":"link","label":"GoogLeNet\uff1a\u81f4\u656cLeNet","href":"/ch2p2/[9]GoogLeNet"},{"type":"link","label":"GoogLeNet\u4ee3\u7801\u5b9e\u73b0","href":"/ch2p2/[10]GoogLeNet-code"},{"type":"link","label":"ResNet\uff1a\u6b8b\u5dee\u7f51\u7edc","href":"/ch2p2/[11]ResNet"},{"type":"link","label":"ResNet\u4ee3\u7801\u5b9e\u73b0","href":"/ch2p2/[12]ResNet-code"}]},{"type":"category","label":"\u7b2c\u4e09\u7ae0\u4e0a\uff1a\u8c08\u4e00\u4e9b\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u5411","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u6df1\u5ea6\u5b66\u4e60\u4e4b\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9","href":"/ch3p1/[1]deep-learning-for-computer-vision"},{"type":"link","label":"\u76ee\u6807\u68c0\u6d4b\u8bba\u6587\u7efc\u8ff0","href":"/ch3p1/[2]overview-of-target-detection"},{"type":"link","label":"\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7efc\u8ff0","href":"/ch3p1/[3]overview-of-semantic-segmentation"},{"type":"link","label":"\u98ce\u683c\u8fc1\u79fb","href":"/ch3p1/[4]image-style-transfer"}]},{"type":"category","label":"\u7b2c\u4e09\u7ae0\u4e0b\uff1a\u4e86\u89e3\u66f4\u9ad8\u7ea7\u7684\u6280\u672f","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u8fb9\u754c\u6846\u548c\u951a\u6846","href":"/ch3p2/[1]bounding-box-and-anchor-box"},{"type":"link","label":"\u56fe\u50cf\u589e\u5e7f","href":"/ch3p2/[2]image-augmentation"},{"type":"link","label":"\u7f16\u7801\u5668-\u89e3\u7801\u5668","href":"/ch3p2/[3]encoder-decoder"},{"type":"link","label":"\u6ce8\u610f\u529b\u673a\u5236","href":"/ch3p2/[4]attention"},{"type":"link","label":"\u5d4c\u5165\u7a7a\u95f4(Embedding)\u7684\u7406\u89e3","href":"/ch3p2/[5]embedding-space"},{"type":"link","label":"\u8f7b\u91cf\u5316\u7f51\u7edc\u8bbe\u8ba1","href":"/ch3p2/[6]light-weight-network-design"},{"type":"link","label":"Transformer\u5230Vision Transformer","href":"/ch3p2/[7]vision-transformer"}]},{"type":"category","label":"\u9644\u5f551\uff1a\u597d\u670b\u53cb\u4eec","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u673a\u5668\u5b66\u4e60(?)\u672f\u8bed\u8868","href":"/appendix-1/[1]similar-vocabularies"},{"type":"link","label":"\u6fc0\u6d3b\u51fd\u6570\u4eec","href":"/appendix-1/[2]activation-functions"},{"type":"link","label":"\u635f\u5931\u51fd\u6570\u4eec","href":"/appendix-1/[3]loss-functions"},{"type":"link","label":"\u773c\u719f\u7684\u4ee3\u7801\u5757","href":"/appendix-1/[4]similar-codeblocks"},{"type":"link","label":"\u642c\u7816\u65f6\u5e94\u8be5\u638c\u63e1\u7684matplotlib","href":"/appendix-1/[5]introducing-matplotlib"},{"type":"link","label":"\u5de5\u5177\u7bb1\uff1f","href":"/appendix-1/[6]who-is-akasaki-toolbox"}]},{"type":"category","label":"\u9644\u5f552\uff1a\u6570\u5b66\u662f\u771f\u6b63\u7684\u5723\u7ecf","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u8d1d\u53f6\u65af\u65b9\u6cd5","href":"/appendix-2/[1]bayesian-methods"}]},{"type":"category","label":"\u9644\u5f553\uff1a\u4fe1\u53f7\u548c\u91c7\u6837\u7684\u5b66\u95ee\uff08DSP\uff09","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"\u5173\u4e8e\u6570\u5b57\u4fe1\u53f7\u5904\u7406","href":"/appendix-3/[1]about-dsp"},{"type":"link","label":"\u5468\u671f\u4fe1\u53f7","href":"/appendix-3/[2]periodic-signal"},{"type":"link","label":"\u56fe\u50cf\u4e2d\u7684\u4fe1\u53f7","href":"/appendix-3/[3]signal-in-images"}]},{"type":"category","label":"\u9644\u5f554\uff1aTensorFlow\u7f16\u7a0b\u7b56\u7565","collapsed":true,"collapsible":true,"items":[{"type":"link","label":"Tensor\u7684\u57fa\u672c\u64cd\u4f5c\uff1a\u7d22\u5f15\u3001\u5207\u7247\u548c\u5e7f\u64ad","href":"/appendix-4/[1]operation-on-tensors-1"},{"type":"link","label":"Tensor\u7684\u57fa\u672c\u64cd\u4f5c\uff1a\u5408\u5e76\u3001\u5206\u5272\u4ee5\u53ca\u7edf\u8ba1","href":"/appendix-4/[2]operation-on-tensors-2"},{"type":"link","label":"\u5f20\u91cf\u7684\u57fa\u672c\u6570\u5b66\u8fd0\u7b97","href":"/appendix-4/[3]operator-for-tensors"},{"type":"link","label":"TensorFlow\u7f16\u7a0b\u7b56\u7565","href":"/appendix-4/[4]tensorflow-strategy"}]},{"type":"link","label":"\u9b54\u6cd5\u90e8\u65e5\u5fd7\uff08\u53c8\u540d\u8bba\u6587\u9605\u8bfb\u65e5\u5fd7\uff09","href":"/unlimited-paper-works/"}],"upwSidebar":[{"type":"link","label":"\u5de5\u5177\u7bb1\u7684\u6df1\u5ea6\u5b66\u4e60\u8bb0\u4e8b\u7c3f","href":"/"},{"type":"category","label":"\u9b54\u6cd5\u90e8\u65e5\u5fd7\uff08\u53c8\u540d\u8bba\u6587\u9605\u8bfb\u65e5\u5fd7\uff09","collapsed":false,"collapsible":false,"items":[{"type":"link","label":"\u6b22\u8fce\u6765\u5230\u9b54\u6cd5\u90e8\u65e5\u5fd7","href":"/unlimited-paper-works/"},{"type":"link","label":"The Devil is in the Decoder: Classification, Regression and GANs","href":"/unlimited-paper-works/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs"},{"type":"link","label":"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey","href":"/unlimited-paper-works/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey"},{"type":"link","label":"Progressive Semantic Segmentation","href":"/unlimited-paper-works/[03]Progressive-Semantic-Segmentation"},{"type":"link","label":"Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation","href":"/unlimited-paper-works/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation"},{"type":"link","label":"HLA-Face Joint High-Low Adaptation for Low Light Face Detection","href":"/unlimited-paper-works/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"},{"type":"link","label":"DeepLab Series","href":"/unlimited-paper-works/[06]DeepLab-Series"},{"type":"link","label":"Cross-Dataset Collaborative Learning for Semantic Segmentation","href":"/unlimited-paper-works/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation"},{"type":"link","label":"Dynamic Neural Networks: A Survey","href":"/unlimited-paper-works/[08]Dynamic-Neural-Networks-A-Survey"},{"type":"link","label":"Feature Pyramid Networks for Object Detection","href":"/unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection"},{"type":"link","label":"A Review on Deep Learning Techniques Applied to Semantic Segmentation","href":"/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation"},{"type":"link","label":"Image Segmentation Using Deep Learning: A Survey","href":"/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey"},{"type":"link","label":"MobileNetV2: Inverted Residuals and Linear Bottlenecks","href":"/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck"},{"type":"link","label":"Fast-SCNN: Fast Semantic Segmentation Network","href":"/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network"},{"type":"link","label":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","href":"/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"},{"type":"link","label":"Gated Channel Transformation for Visual Recognition","href":"/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition"},{"type":"link","label":"Convolutional Block Attention Module","href":"/unlimited-paper-works/[16]Convolutional-Block-Attention-Module"},{"type":"link","label":"Boundary IoU: Improving Object-Centric Image Segmentation Evaluation","href":"/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation"},{"type":"link","label":"Involution: Inverting the Inherence of Convolution for Visual Recognition","href":"/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition"},{"type":"link","label":"PointRend: Image Segmentation as Rendering","href":"/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering"},{"type":"link","label":"Transformer: Attention is all you need","href":"/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need"},{"type":"link","label":"RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features","href":"/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features"},{"type":"link","label":"GLADNet: Low-Light Enhancement Network with Global Awareness","href":"/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness"},{"type":"link","label":"Squeeze-and-Excitation Networks (SENet)","href":"/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks"},{"type":"link","label":"BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation","href":"/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation"},{"type":"link","label":"Rethinking BiSeNet For Real-time Semantic Segmentation","href":"/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation"},{"type":"link","label":"CBAM: Convolutional Block Attention Module","href":"/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module"},{"type":"link","label":"Non-local Neural Networks","href":"/unlimited-paper-works/[27]Non-local-Neural-Networks"},{"type":"link","label":"GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)","href":"/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond"},{"type":"link","label":"Disentangled Non-Local Neural Networks","href":"/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks"},{"type":"link","label":"Deep Retinex Decomposition for Low-Light Enhancement","href":"/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement"},{"type":"link","label":"MSR-net:Low-light Image Enhancement Using Deep Convolutional Network","href":"/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network"},{"type":"link","label":"LLCNN: A convolutional neural network for low-light image enhancement","href":"/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"},{"type":"link","label":"VOLO: Vision Outlooker for Visual Recognition","href":"/unlimited-paper-works/[33]VOLO-Vision-Outlooker-for-Visual-Recognition"},{"type":"link","label":"Polarized Self-Attention: Towards High-quality Pixel-wise Regression","href":"/unlimited-paper-works/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression"},{"type":"link","label":"SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks","href":"/unlimited-paper-works/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks"},{"type":"link","label":"SOLO: Segmenting Objects by Locations","href":"/unlimited-paper-works/[36]SOLO-Segmenting-Objects-by-Locations"},{"type":"link","label":"YOLACT: Real-time Instance Segmentation","href":"/unlimited-paper-works/[37]YOLACT-Real-time-Instance-Segmentation"},{"type":"link","label":"You Only Look One-level Feature","href":"/unlimited-paper-works/[38]You-Only-Look-One-level-Feature"},{"type":"link","label":"Instance-sensitive Fully Convolutional Networks","href":"/unlimited-paper-works/[39]Instance-sensitive-Fully-Convolutional-Networks"},{"type":"link","label":"Learning in the Frequency Domain","href":"/unlimited-paper-works/[40]Learning-in-the-Frequency-Domain"},{"type":"link","label":"Generative Adversarial Networks","href":"/unlimited-paper-works/[41]Generative-Adversarial-Networks"},{"type":"link","label":"[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation","href":"/unlimited-paper-works/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation"},{"type":"link","label":"[43]RepVGG-Making-VGG-style-ConvNets-Great-Again","href":"/unlimited-paper-works/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again"},{"type":"link","label":"[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network","href":"/unlimited-paper-works/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network"},{"type":"link","label":"[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows","href":"/unlimited-paper-works/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"},{"type":"link","label":"[46]Demystifying-Local-Vision-Transformer","href":"/unlimited-paper-works/[46]Demystifying-Local-Vision-Transformer"}]}]}}')}}]);