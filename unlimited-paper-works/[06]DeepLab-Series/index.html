<!doctype html>
<html class="docs-version-current" lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">DeepLab Series | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//unlimited-paper-works/[06]DeepLab-Series"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-docs-current"><meta data-react-helmet="true" property="og:title" content="DeepLab Series | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="description" content="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯VisualDustã€‚"><meta data-react-helmet="true" property="og:description" content="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯VisualDustã€‚"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//unlimited-paper-works/[06]DeepLab-Series"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[06]DeepLab-Series" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[06]DeepLab-Series" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.54fd31ab.css">
<link rel="preload" href="/assets/js/runtime~main.e23c63c0.js" as="script">
<link rel="preload" href="/assets/js/main.f6709b37.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/unlimited-paper-works/">é­”æ³•éƒ¨æ—¥å¿—</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menuLinkText_OKON">é­”æ³•éƒ¨æ—¥å¿—ï¼ˆåˆåè®ºæ–‡é˜…è¯»æ—¥å¿—ï¼‰</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/unlimited-paper-works/[06]DeepLab-Series">DeepLab Series</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey">Image Segmentation Using Deep Learning: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering">PointRend: Image Segmentation as Rendering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need">Transformer: Attention is all you need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks (SENet)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module">CBAM: Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN: A convolutional neural network for low-light image enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO: Vision Outlooker for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[36]SOLO-Segmenting-Objects-by-Locations">SOLO: Segmenting Objects by Locations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT: Real-time Instance Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[41]Generative-Adversarial-Networks">Generative Adversarial Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">[43]RepVGG-Making-VGG-style-ConvNets-Great-Again</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[46]Demystifying-Local-Vision-Transformer">[46]Demystifying-Local-Vision-Transformer</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_eoK2"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_e+kA"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>DeepLab Series</h1></header><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>ã€‚<a aria-hidden="true" class="hash-link" href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust" title="Direct link to heading">â€‹</a></h3><p>DeepLabç³»åˆ—ä¸­åŒ…å«äº†ä¸‰ç¯‡è®ºæ–‡ï¼šDeepLab-v1ã€DeepLab-v2ã€DeepLab-v3ã€‚</p><p>DeepLab-v1ï¼š<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener noreferrer">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>DeepLab-v2ï¼š<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener noreferrer">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>DeepLab-v3ï¼š<a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener noreferrer">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>åœ¨è¿™é‡Œæˆ‘ä»¬å°†è¿™ä¸‰ç¯‡æ”¾åœ¨ä¸€èµ·é˜…è¯»ã€‚</p><p>åæ¥ç”šè‡³è¿˜å‡ºç°äº†åç»­ï¼š</p><p>DeepLab-v3+ï¼š<a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener noreferrer">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>ä¸è¿‡æš‚æ—¶æ²¡æœ‰å†™è¿›æ¥çš„æ‰“ç®—ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="deeplab-v1">DeepLab-v1<a aria-hidden="true" class="hash-link" href="#deeplab-v1" title="Direct link to heading">â€‹</a></h2><p>DeepLab-v1çš„åŸè®ºæ–‡æ˜¯<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener noreferrer">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a>ã€‚</p><blockquote><p>In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or &#x27;atrous convolution&#x27;, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed &quot;DeepLab&quot; system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.</p></blockquote><p>åœ¨ä¹‹å‰çš„è¯­ä¹‰åˆ†å‰²ç½‘ç»œä¸­ï¼Œåˆ†å‰²ç»“æœå¾€å¾€æ¯”è¾ƒç²—ç³™ï¼ŒåŸå› ä¸»è¦æœ‰ä¸¤ä¸ªï¼Œä¸€æ˜¯å› ä¸ºæ± åŒ–å¯¼è‡´ç©ºé—´ä¿¡æ¯ä¸¢å¤±ï¼ŒäºŒæ˜¯æ²¡æœ‰åˆ©ç”¨ä¸´è¿‘åƒç´ ç‚¹ç±»åˆ«ä¹‹é—´çš„æ¦‚ç‡å…³ç³»ï¼Œé’ˆå¯¹è¿™ä¸¤ç‚¹ï¼Œä½œè€…æå‡ºäº†é’ˆå¯¹æ€§çš„æ”¹è¿›ã€‚é¦–å…ˆä½¿ç”¨<strong>ç©ºæ´å·ç§¯ï¼ˆAtrous Convolutionï¼‰</strong>ï¼Œé¿å…æ± åŒ–å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ï¼Œç„¶åä½¿ç”¨<strong>æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰</strong>ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åˆ†å‰²ç²¾åº¦ã€‚é˜…è¯»è¿™ç¯‡è®ºæ–‡åº”å…³æ³¨çš„é‡ç‚¹é—®é¢˜å°±æ˜¯ç©ºæ´å·ç§¯å’Œæ¡ä»¶éšæœºåœºã€‚</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ç©ºæ´å·ç§¯">ç©ºæ´å·ç§¯<a aria-hidden="true" class="hash-link" href="#ç©ºæ´å·ç§¯" title="Direct link to heading">â€‹</a></h3><p>ç©ºæ´å·ç§¯ï¼ˆDilated/Atrous Convolutionæˆ–æ˜¯Convolution with holes ï¼‰çš„ä¸»è¦ä½œç”¨æ˜¯åœ¨å¢å¤§æ„Ÿå—é‡çš„åŒæ—¶ï¼Œä¸å¢åŠ å‚æ•°æ•°é‡ï¼Œè€Œä¸”VGGä¸­æå‡ºçš„å¤šä¸ªå°å·ç§¯æ ¸ä»£æ›¿å¤§å·ç§¯æ ¸çš„æ–¹æ³•ï¼Œåªèƒ½ä½¿æ„Ÿå—é‡çº¿æ€§å¢é•¿ï¼Œè€Œå¤šä¸ªç©ºæ´å·ç§¯ä¸²è”ï¼Œå¯ä»¥å®ç°æŒ‡æ•°å¢é•¿ã€‚</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="ç©ºæ´å·ç§¯çš„ä¼˜åŠ¿">ç©ºæ´å·ç§¯çš„ä¼˜åŠ¿<a aria-hidden="true" class="hash-link" href="#ç©ºæ´å·ç§¯çš„ä¼˜åŠ¿" title="Direct link to heading">â€‹</a></h4><ul><li>è¿™ç§ç»“æ„ä»£æ›¿äº†æ± åŒ–ï¼Œå®ƒå¯ä»¥ä¿æŒåƒç´ ç©ºé—´ä¿¡æ¯ã€‚</li><li>å®ƒç”±äºå¯ä»¥æ‰©å¤§æ„Ÿå—é‡å› è€Œå¯ä»¥å¾ˆå¥½åœ°æ•´åˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li></ul><p>Convolution with holes å­—å¦‚å…¶åï¼Œæ˜¯åœ¨æ ‡å‡†çš„å·ç§¯æ ¸ä¸­æ³¨å…¥ç©ºæ´ï¼Œä»¥æ­¤æ¥å¢åŠ æ„Ÿå—é‡ã€‚ç›¸æ¯”äºæ™®é€šçš„å·ç§¯ï¼Œç©ºæ´å·ç§¯å¤šäº†ä¸€ä¸ªè¶…å‚æ•°ç§°ä¹‹ä¸ºç©ºæ´ç‡ï¼ˆdilation rateï¼‰æŒ‡çš„æ˜¯kernelçš„é—´éš”çš„åƒç´ æ•°é‡ã€‚</p><p><img alt="Atrous_conv" src="/assets/images/Atrous_conv-8bbc9b569825f67962e113f732b01b61.png"></p><p>ä¸Šå›¾æ˜¯ä¸€å¼ ç©ºæ´å·ç§¯çš„ç¤ºæ„å›¾ã€‚åœ¨ä¸Šå›¾ä¸­ï¼Œä¸‰ä¸ªç©ºæ´å·ç§¯çš„å¤§å°éƒ½æ˜¯<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span></span>ï¼Œè€Œå®ƒä»¬çš„ç©ºæ´ç‡åˆ†åˆ«æ˜¯1ã€6å’Œ24ï¼Œæ‰€ä»¥èƒ½ç”¨ç›¸åŒå¤§å°çš„å·ç§¯æ ¸å¾—åˆ°ä¸åŒçš„æ„Ÿå—é‡ã€‚</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="ç©ºæ´å·ç§¯çš„é—®é¢˜">ç©ºæ´å·ç§¯çš„é—®é¢˜<a aria-hidden="true" class="hash-link" href="#ç©ºæ´å·ç§¯çš„é—®é¢˜" title="Direct link to heading">â€‹</a></h4><ul><li><p>ç½‘æ ¼æ•ˆåº”ï¼ˆThe Gridding Effectï¼‰</p><p>ç©ºæ´å·ç§¯å±‚å¹¶ä¸èƒ½éšæ„è®¾è®¡ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬ç®€å•åœ°å †å ç©ºæ´ç‡ä¸º2çš„<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>Ã—</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span></span>çš„ç©ºæ´å·ç§¯æ ¸ï¼Œé‚£ä¹ˆè¿ç»­ä¸‰å±‚å·ç§¯æ ¸åœ¨åŸå›¾ä¸Šçš„åŒä¸ªåƒç´ ä½ç½®æ‰€å¯¹åº”çš„æ„Ÿå—é‡å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p><p><img alt="image-20210514145720970" src="/assets/images/image-20210514145720970-e129c46568b26824bc4946847ff0323c.png"></p><p>å¾ˆæ˜æ˜¾ï¼Œæ ‡åœ†åœˆçš„ä½ç½®ä¸€ç›´æ²¡æœ‰å‚ä¸è¯¥ä½ç½®çš„å·ç§¯è¿ç®—ã€‚ä¹Ÿå°±æ˜¯å¹¶ä¸æ˜¯æ‰€æœ‰çš„åƒç´ éƒ½ç”¨æ¥è®¡ç®—äº†ï¼Œè¿™ä¼šå¯¼è‡´ä¿¡æ¯çš„è¿ç»­æ€§æŸå¤±ã€‚è¿™å¯¹å¯†é›†é¢„æµ‹ï¼ˆé€åƒç´ ï¼‰çš„è§†è§‰ä»»åŠ¡æ¥è¯´æ˜¯è‡´å‘½çš„ã€‚</p></li><li><p>ç›¸å…³æ€§ä¸¢å¤±</p><p>åŸè®ºæ–‡ä¸­æè¿°é—®é¢˜çš„è¯æ˜¯ï¼š</p><blockquote><p>Long-ranged information might be not relevant.</p></blockquote><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä» dilated convolution çš„è®¾è®¡èƒŒæ™¯æ¥çœ‹å°±èƒ½æ¨æµ‹å‡ºè¿™æ ·çš„è®¾è®¡æ˜¯ç”¨æ¥è·å– long-ranged informationã€‚ç„¶è€Œä»…é‡‡ç”¨å¤§ dilation rate çš„ä¿¡æ¯æˆ–è®¸åªå¯¹ä¸€äº›å¤§ç‰©ä½“åˆ†å‰²æœ‰æ•ˆæœï¼Œè€Œå¯¹å°ç‰©ä½“æ¥è¯´å¯èƒ½åˆ™æœ‰å¼Šæ— åˆ©äº†ã€‚å¦‚ä½•åŒæ—¶å¤„ç†ä¸åŒå¤§å°çš„ç‰©ä½“çš„å…³ç³»ï¼Œåˆ™æ˜¯è®¾è®¡å¥½ dilated convolution ç½‘ç»œçš„å…³é”®ã€‚</p></li></ul><h4 class="anchor anchorWithStickyNavbar_y2LR" id="æ··åˆè†¨èƒ€å·ç§¯hybrid-dilated-convolution-hdc">æ··åˆè†¨èƒ€å·ç§¯ï¼ˆHybrid Dilated Convolution, HDCï¼‰<a aria-hidden="true" class="hash-link" href="#æ··åˆè†¨èƒ€å·ç§¯hybrid-dilated-convolution-hdc" title="Direct link to heading">â€‹</a></h4><p>å¯¹äºåˆšæ‰æåˆ°çš„ç©ºæ´å·ç§¯çš„é—®é¢˜ï¼Œè®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§ç§°ä¸ºHDCçš„ç»“æ„ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚è¿™ä¸ªæ–¹æ¡ˆå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š</p><ul><li>å¯¹äºæ¯å±‚ç©ºæ´å·ç§¯ï¼Œå…¶æœ€å¤§ç©ºæ´å·ç§¯ç‡çš„æœ€å°å…¬å› å­ä¸èƒ½ä¸º1ã€‚</li><li></li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="æ¡ä»¶éšæœºåœº">æ¡ä»¶éšæœºåœº<a aria-hidden="true" class="hash-link" href="#æ¡ä»¶éšæœºåœº" title="Direct link to heading">â€‹</a></h3><p>æ¡ä»¶éšæœºåœºï¼Œç®€å•æ¥è®²å°±æ˜¯æ¯ä¸ªåƒç´ ç‚¹ä½œä¸ºèŠ‚ç‚¹ï¼Œåƒç´ ä¸åƒç´ é—´çš„å…³ç³»ä½œä¸ºè¾¹ï¼Œå³æ„æˆäº†ä¸€ä¸ªæ¡ä»¶éšæœºåœºã€‚é€šè¿‡äºŒå…ƒåŠ¿å‡½æ•°æè¿°åƒç´ ç‚¹ä¸åƒç´ ç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œé¼“åŠ±ç›¸ä¼¼åƒç´ åˆ†é…ç›¸åŒçš„æ ‡ç­¾ï¼Œè€Œç›¸å·®è¾ƒå¤§çš„åƒç´ åˆ†é…ä¸åŒæ ‡ç­¾ï¼Œè€Œè¿™ä¸ªâ€œè·ç¦»â€çš„å®šä¹‰ä¸é¢œè‰²å€¼å’Œå®é™…ç›¸å¯¹è·ç¦»æœ‰å…³ã€‚æ‰€ä»¥è¿™æ ·CRFèƒ½å¤Ÿä½¿å›¾ç‰‡åœ¨åˆ†å‰²çš„è¾¹ç•Œå‡ºå–å¾—æ¯”è¾ƒå¥½çš„æ•ˆæœã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="deeplab-v2">DeepLab-v2<a aria-hidden="true" class="hash-link" href="#deeplab-v2" title="Direct link to heading">â€‹</a></h2><p>DeepLab-v2çš„åŸè®ºæ–‡æ˜¯<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener noreferrer">Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a>ã€‚</p><blockquote><p>Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called &quot;semantic image segmentation&quot;). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our &quot;DeepLab&quot; system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the &#x27;hole&#x27; algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.</p></blockquote><p>DeepLab-v2å¯¹DeepLab-v1çš„æ”¹è¿›æ˜¯ï¼š</p><ul><li>ä½¿ç”¨äº†é‡‘å­—å¡”å¤šå°ºåº¦ç‰¹å¾è·å¾—æ›´å¥½çš„åˆ†å‰²æ•ˆæœã€‚</li><li>å°†éª¨å¹²ç½‘ç»œç”±VGGæ›¿æ¢ä¸ºäº†ResNetã€‚</li><li>ç¨å¾®ä¿®æ”¹äº†learning-rateã€‚</li></ul><p>å…¶ä¸­ASPPçš„å¼•å…¥æ˜¯æœ€å¤§ä¹Ÿæ˜¯æœ€é‡è¦çš„æ”¹å˜ã€‚å¤šå°ºåº¦ä¸»è¦æ˜¯ä¸ºäº†è§£å†³ç›®æ ‡åœ¨å›¾åƒä¸­è¡¨ç°ä¸ºä¸åŒå¤§å°æ—¶ä»èƒ½å¤Ÿæœ‰å¾ˆå¥½çš„åˆ†å‰²ç»“æœï¼Œæ¯”å¦‚åŒæ ·çš„ç‰©ä½“ï¼Œåœ¨è¿‘å¤„æ‹æ‘„æ—¶ç‰©ä½“æ˜¾å¾—å¤§ï¼Œè¿œå¤„æ‹æ‘„æ—¶æ˜¾å¾—å°ã€‚å…·ä½“åšæ³•æ˜¯å¹¶è¡Œçš„é‡‡ç”¨å¤šä¸ªé‡‡æ ·ç‡çš„ç©ºæ´å·ç§¯æå–ç‰¹å¾ï¼Œå†å°†ç‰¹å¾èåˆï¼Œç±»ä¼¼äºç©ºé—´é‡‘å­—å¡”ç»“æ„ï¼Œå½¢è±¡çš„ç§°ä¸ºAtrous Spatial Pyramid Pooling (ASPP)ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="deeplab-v3">DeepLab-v3<a aria-hidden="true" class="hash-link" href="#deeplab-v3" title="Direct link to heading">â€‹</a></h2><p>DeepLab-v3çš„åŸè®ºæ–‡æ˜¯<a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener noreferrer">Rethinking Atrous Convolution for Semantic Image Segmentation</a>ã€‚</p><blockquote><p>In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter&#x27;s field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3&#x27; system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.</p></blockquote><p>DeepLab-v3çš„æ”¹è¿›æ˜¯ï¼š</p><ul><li>æå‡ºäº†æ›´é€šç”¨çš„æ¡†æ¶ï¼Œé€‚ç”¨äºä»»ä½•ç½‘ç»œã€‚</li><li>å°†ResNetæœ€åçš„ä¸€äº›æ¨¡å—æ›¿æ¢ä¸ºä½¿ç”¨ç©ºæ´å·ç§¯è¿›è¡Œçš„çº§è”ã€‚</li><li>åœ¨ASPPä¸­ä½¿ç”¨äº†Batch Normolizationå±‚ã€‚</li><li>å»é™¤äº†æ¡ä»¶éšæœºåœºã€‚</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/unlimited-paper-works/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->HLA-Face Joint High-Low Adaptation for Low Light Face Detection</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/unlimited-paper-works/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Cross-Dataset Collaborative Learning for Semantic Segmentation<!-- --> Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust" class="table-of-contents__link toc-highlight">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯VisualDustã€‚</a></li><li><a href="#deeplab-v1" class="table-of-contents__link toc-highlight">DeepLab-v1</a><ul><li><a href="#ç©ºæ´å·ç§¯" class="table-of-contents__link toc-highlight">ç©ºæ´å·ç§¯</a></li><li><a href="#æ¡ä»¶éšæœºåœº" class="table-of-contents__link toc-highlight">æ¡ä»¶éšæœºåœº</a></li></ul></li><li><a href="#deeplab-v2" class="table-of-contents__link toc-highlight">DeepLab-v2</a></li><li><a href="#deeplab-v3" class="table-of-contents__link toc-highlight">DeepLab-v3</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e23c63c0.js"></script>
<script src="/assets/js/main.f6709b37.js"></script>
</body>
</html>