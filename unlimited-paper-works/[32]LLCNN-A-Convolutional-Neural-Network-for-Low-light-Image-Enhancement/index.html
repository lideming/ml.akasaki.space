<!doctype html>
<html class="docs-version-current" lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">LLCNN: A convolutional neural network for low-light image enhancement | 工具箱的深度学习记事簿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-docs-current"><meta data-react-helmet="true" property="og:title" content="LLCNN: A convolutional neural network for low-light image enhancement | 工具箱的深度学习记事簿"><meta data-react-helmet="true" name="description" content="论文名称 A convolutional neural network for low-light image enhancement"><meta data-react-helmet="true" property="og:description" content="论文名称 A convolutional neural network for low-light image enhancement"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.54fd31ab.css">
<link rel="preload" href="/assets/js/runtime~main.e23c63c0.js" as="script">
<link rel="preload" href="/assets/js/main.f6709b37.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/unlimited-paper-works/">魔法部日志</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">工具箱的深度学习记事簿</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menuLinkText_OKON">魔法部日志（又名论文阅读日志）</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/">欢迎来到魔法部日志</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[06]DeepLab-Series">DeepLab Series</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey">Image Segmentation Using Deep Learning: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering">PointRend: Image Segmentation as Rendering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need">Transformer: Attention is all you need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks (SENet)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module">CBAM: Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN: A convolutional neural network for low-light image enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO: Vision Outlooker for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[36]SOLO-Segmenting-Objects-by-Locations">SOLO: Segmenting Objects by Locations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT: Real-time Instance Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[41]Generative-Adversarial-Networks">Generative Adversarial Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">[43]RepVGG-Making-VGG-style-ConvNets-Great-Again</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[46]Demystifying-Local-Vision-Transformer">[46]Demystifying-Local-Vision-Transformer</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_eoK2"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_e+kA"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>LLCNN: A convolutional neural network for low-light image enhancement</h1></header><blockquote><p>论文名称: <a href="https://ieeexplore.ieee.org/abstract/document/8305143/" target="_blank" rel="noopener noreferrer">LLCNN: A convolutional neural network for low-light image enhancement</a></p><p>论文作者: Li Tao, Chuang Zhu, Guoqing Xiang, Yuan Li, Huizhu Jia, Xiaodong Xie</p><p>Code: <a href="https://github.com/BestJuly/LLCNN" target="_blank" rel="noopener noreferrer">https://github.com/BestJuly/LLCNN</a></p></blockquote><h3 class="anchor anchorWithStickyNavbar_y2LR" id="这篇笔记的写作者是pommespeter">这篇笔记的写作者是<a href="https://github.com/PommesPeter" target="_blank" rel="noopener noreferrer">PommesPeter</a>。<a aria-hidden="true" class="hash-link" href="#这篇笔记的写作者是pommespeter" title="Direct link to heading">​</a></h3><p>这是一篇讲解使用卷积神经网络进行低照度增强的论文。</p><ul><li>本文使用卷积神经网络进行低照度增强</li><li>使用SSIM损失更好地评价图像好坏和梯度收敛</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract-摘要">Abstract (摘要)<a aria-hidden="true" class="hash-link" href="#abstract-摘要" title="Direct link to heading">​</a></h2><blockquote><p>In this paper, we propose a CNN based method to perform low-light image enhancement. We design a special  module to utilize multiscale feature maps, which can avoid  gradient vanishing problem as well. In order to preserve image textures as much as possible, we use SSIM loss to train our model. The contrast of low-light images can be adaptively enhanced using our method. Results demonstrate that our CNN based method  outperforms other contrast enhancement methods. </p></blockquote><p>本文提出了一种基于CNN的低照度图像增强方法。我们设计了一个特殊的模块来<strong>利用多尺度特征映射</strong>，这样可以避免梯度消失的问题。<strong>为了尽可能地保留图像纹理，我们使用SSIM损失来训练我们的模型</strong>。使用我们的方法可以<strong>自适应地增强弱光图像的对比度</strong>。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="introduction">Introduction<a aria-hidden="true" class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>为了增强图像对比度和提高图像亮度，提出了几种算法。<strong>直方图均衡(HE)方法</strong>[1]<!-- -->，<!-- -->[2]<!-- -->重新排列像素值，使其服从均匀分布。<strong>基于视网膜理论的方法</strong>[3]<!-- -->–<!-- -->[7]<!-- -->利用了一个模型，该模型假设图像是照明和反射的相互作用。<strong>基于dehaze模型的方法</strong>[8]<!-- -->固定像素值，使其服从自然分布。我们称所有这些方法为传统方法。</p><p>本文应用CNN对低照度图像进行增强。我们称所提出的网络为低照度卷积神经网络。它学习用不同的核过滤弱光图像，然后将多尺度特征图组合在一起生成增强图像，这些图像看起来是在正常光照条件下捕获的，并且保留了原始特征和纹理。此外，<strong>SSIM损失被整合到我们的LLCNN，以重建更准确的图像纹理。</strong>我们将我们的结果与其他方法进行了比较，结果表明我们的方法在常见的弱光图像增强方法中取得了最好的性能。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="related-work">Related Work<a aria-hidden="true" class="hash-link" href="#related-work" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="低照度增强方法">低照度增强方法<a aria-hidden="true" class="hash-link" href="#低照度增强方法" title="Direct link to heading">​</a></h3><p>一般来说，低照度图像增强方法可以分为<strong>三类</strong>。</p><ul><li><p>HE方法保持像素值之间的相对关系，尽量使其服从均匀分布。</p></li><li><p>动态直方图均衡化(DHE)  <!-- -->[1]<!-- -->将直方图分成几个部分，并在每个子直方图中执行HE处理。</p></li><li><p>对比度受限的自适应直方图均衡化(CLAHE)  <!-- -->[2]<!-- -->自适应地限制了HE的对比度增强结果的程度。</p></li></ul><p>对于这些HE方法，在许多情况下会出现严重的偏色问题，而且暗黑暗区域的细节没有得到适当的增强。</p><p>基于Retinex理论的方法计算图像的照度，并通过去除它们来实现图像增强。单尺度视网膜(SSR)  <!-- -->[3]<!-- -->、多尺度视网膜(MSR) <!-- -->[4]<!-- -->和带颜色恢复的多尺度视网膜(MSRCR) <!-- -->[5]<!-- -->是这类方法的典型作品。最近，一些新的方法(SRIE  <!-- -->[6]<!-- -->，LIME<!-- -->[7]<!-- -->)被提出来估计光照图和反射率以增强低照度图像。在许多情况下，这些方法可能会出现严重的<strong>颜色失真</strong>。</p><p>基于Dehaze模型的方法反转低照度图像，并在其上应用dehaze方法。在Dehaze模型中，这种方法用于增强低照度图像。然而，图像通常被过度增强，并且增强图像的饱和度通常被夸大。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="使用深度学习进行图像处理的方法">使用深度学习进行图像处理的方法<a aria-hidden="true" class="hash-link" href="#使用深度学习进行图像处理的方法" title="Direct link to heading">​</a></h3><p>*<!-- -->LLNet是唯一使用深度神经网络增强低照度图像的方法。该网络是<strong>堆叠稀疏去噪自动编码器的变体</strong>，并且它不使用卷积层。使用非线性方法模拟弱光条件，使自然图像变暗。这些图像被设置为训练数据。经过训练，网络可以增强弱光图像。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="inception模块和残差学习">Inception模块和残差学习<a aria-hidden="true" class="hash-link" href="#inception模块和残差学习" title="Direct link to heading">​</a></h3><p>在许多计算机视觉任务中，深度网络比非深度网络具有更好的性能。然而，当堆叠的卷积层越深，网络就会遇到梯度消失的严重问题，这可能会导致训练难以收敛。使用inception模块和残差学习能够解决这些问题。(因为GoogleNet and ResNet取得了比较好的效果)</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="proposed-method">Proposed Method<a aria-hidden="true" class="hash-link" href="#proposed-method" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="网络结构">网络结构<a aria-hidden="true" class="hash-link" href="#网络结构" title="Direct link to heading">​</a></h3><p>虽然低照度图像增强属于低层次(low-level)的图像处理任务，但它与超分辨率和图像去噪有很大不同。对于这两个任务，劣质图像中的像素值都在真值附近，平均像素值几乎不变，这在我们的任务中是不同的。因此，我们设计了一个不同但有效的CNN网络来增强弱光图像。网络结构如下</p><p><img alt="image-20210709090203976" src="/assets/images/image-20210709090203976-50d85d267315c50b38f2ede75e2e9841.png"></p><p>具体流程如Fig2，可分为两个阶段。</p><p>在第一阶段，数据以两种不同的方式处理。一种方式是1×1卷积层，另一种方式是两个3×3卷积层。我们将它们组合在一起，形成第二阶段的输入。第一阶段类似于初始模块。我们不对结果做concat，而是直接相加它们。</p><p>第二阶段，也有两种方式。第一种方式是使用两个3×3卷积层处理数据，而第二种方式是直接绕过输入数据，这是残差学习中使用残差连接。</p><p><img alt="image-20210709090435335" src="/assets/images/image-20210709090435335-13f2c7b1e56a6ef3aa5d826a54b49381.png"></p><p>对于VDSR和DnCNN，网络会生成一个残差图像，通过将残差图像与原始图像相加来计算最终图像。这是因为对于超分辨率和图像去噪，地面真实和劣化图像之间的差异不是很大。对于弱光图像增强，网络学习残留图像似乎比一点一点地增强图像更困难。因此，我们不使用这种在VDSR或DnCNN的架构。另一个原因是我们已经在模块中利用了残差学习。</p><p>LLCNN的结构是这样描述的:</p><p>一个卷积层做预处理产生归一化的输入，另一个卷积层产生增强图像，在这两层之间放置几个特殊设计的卷积模块。对于每个过滤器，我们使用64个过滤器，除了最后一个。最后一层中使用的滤镜数量取决于颜色通道的数量。</p><p>该网络以弱光图像作为输入，并进行处理，以使输出图像看起来像是在正常光线条件下捕获的。类似于VDSR和DnCNN，我们将原始图像切割成面片。补丁大小设置为41×41。所有输入图像均采用非线性方法生成，以模拟弱光条件。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="ssim损失函数">SSIM损失函数<a aria-hidden="true" class="hash-link" href="#ssim损失函数" title="Direct link to heading">​</a></h3><p>在低照度增强中，我们不需要增强在特定光照环境下的图像。例如，给定在白天拍摄的自然图像，通过添加或减去一个小数字来改变所有像素值是影响很小，在这种情况下，结构几乎不会改变，但使用PSNR函数会有很大的差异。SSIM公式如下</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>S</mi><mi>S</mi><mi>I</mi><mi>M</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mn>2</mn><msub><mi>μ</mi><mi>x</mi></msub><msub><mi>μ</mi><mi>y</mi></msub><mo>+</mo><msub><mi>C</mi><mn>1</mn></msub></mrow><mrow><msubsup><mi>μ</mi><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>μ</mi><mi>y</mi><mn>2</mn></msubsup><mo>+</mo><msub><mi>C</mi><mn>1</mn></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mn>2</mn><msub><mi>σ</mi><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>+</mo><msub><mi>C</mi><mn>2</mn></msub></mrow><mrow><msubsup><mi>σ</mi><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>y</mi><mn>2</mn></msubsup><mo>+</mo><msub><mi>C</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">SSIM(p)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}\cdot\frac{2\sigma_{xy}+C_2}{\sigma_x^2+\sigma_y^2+C_2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">SS</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4294em;vertical-align:-1.0691em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7401em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7401em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0691em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:2.4294em;vertical-align:-1.0691em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7401em"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7401em"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0691em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>SSIM的值域范围为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">(0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>，值为1时表示两张图像完全一样，因此我们用<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>S</mi><mi>S</mi><mi>I</mi><mi>M</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1-SSIM(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">SS</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span>来计算像素的损失。损失函数定义如下</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>l</mi><mrow><mi>s</mi><mi>s</mi><mi>i</mi><mi>m</mi></mrow></msub><mfrac><mn>1</mn><mi>N</mi></mfrac><munder><mo>∑</mo><mrow><mi>p</mi><mo>∈</mo><mi>P</mi></mrow></munder><mn>1</mn><mo>−</mo><mi>S</mi><mi>S</mi><mi>I</mi><mi>M</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">l_{ssim}\frac{1}{N}\sum_{p\in P}1-SSIM(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.7519em;vertical-align:-1.4304em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span><span class="mord mathnormal mtight">im</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8557em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4304em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">SS</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span></div><p>我们遵循<!-- -->[11]<!-- -->并将自然图像设置为地面真实图像，并且通过退化方法产生弱光图像。随机伽玛调整用于模拟弱光图像。参数γ在范围(2，5)内随机设置，这将使网络能够自适应地增强图像。我们测试了两种不同深度的网络。一个叫做LLCNN，它有三个模块，所以它有17个卷积层。另一个LLCNN-s使用两个模块，卷积层数为12层。除了最后一层，我们在每层设置了64个过滤器。训练时动量设置为0.9，重量衰减为0.0001。基础学习率为0.01。学习速度会在训练过程中发生变化。这里还提供了SSIM损耗层的参数。卷积核大小和<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">C_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>,<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">C_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>分别设置为8、0.0001和0.001。在我们的训练过程中，每迭代4万次表示1个epoch。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="experiments">Experiments<a aria-hidden="true" class="hash-link" href="#experiments" title="Direct link to heading">​</a></h2><p>详见原文</p><p><img alt="image-20210709091917127" src="/assets/images/image-20210709091917127-a99bb1fbae5ee3437aa1749620bcc3c6.png"></p><p><img alt="image-20210709091927771" src="/assets/images/image-20210709091927771-e0fb01ea1e7dec58d91e9dbebb81418f.png"></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« <!-- -->MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/unlimited-paper-works/[33]VOLO-Vision-Outlooker-for-Visual-Recognition"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">VOLO: Vision Outlooker for Visual Recognition<!-- --> »</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#这篇笔记的写作者是pommespeter" class="table-of-contents__link toc-highlight">这篇笔记的写作者是PommesPeter。</a></li><li><a href="#abstract-摘要" class="table-of-contents__link toc-highlight">Abstract (摘要)</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#related-work" class="table-of-contents__link toc-highlight">Related Work</a><ul><li><a href="#低照度增强方法" class="table-of-contents__link toc-highlight">低照度增强方法</a></li><li><a href="#使用深度学习进行图像处理的方法" class="table-of-contents__link toc-highlight">使用深度学习进行图像处理的方法</a></li><li><a href="#inception模块和残差学习" class="table-of-contents__link toc-highlight">Inception模块和残差学习</a></li></ul></li><li><a href="#proposed-method" class="table-of-contents__link toc-highlight">Proposed Method</a><ul><li><a href="#网络结构" class="table-of-contents__link toc-highlight">网络结构</a></li><li><a href="#ssim损失函数" class="table-of-contents__link toc-highlight">SSIM损失函数</a></li></ul></li><li><a href="#experiments" class="table-of-contents__link toc-highlight">Experiments</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e23c63c0.js"></script>
<script src="/assets/js/main.f6709b37.js"></script>
</body>
</html>