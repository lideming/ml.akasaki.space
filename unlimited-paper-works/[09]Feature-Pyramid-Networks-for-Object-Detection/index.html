<!doctype html>
<html class="docs-version-current" lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">Feature Pyramid Networks for Object Detection | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-docs-current"><meta data-react-helmet="true" property="og:title" content="Feature Pyramid Networks for Object Detection | å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿"><meta data-react-helmet="true" name="description" content="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯VisualDustã€‚"><meta data-react-helmet="true" property="og:description" content="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯VisualDustã€‚"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.54fd31ab.css">
<link rel="preload" href="/assets/js/runtime~main.e23c63c0.js" as="script">
<link rel="preload" href="/assets/js/main.f6709b37.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/unlimited-paper-works/">é­”æ³•éƒ¨æ—¥å¿—</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ğŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ğŸŒ</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">å·¥å…·ç®±çš„æ·±åº¦å­¦ä¹ è®°äº‹ç°¿</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menuLinkText_OKON">é­”æ³•éƒ¨æ—¥å¿—ï¼ˆåˆåè®ºæ–‡é˜…è¯»æ—¥å¿—ï¼‰</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/">æ¬¢è¿æ¥åˆ°é­”æ³•éƒ¨æ—¥å¿—</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[06]DeepLab-Series">DeepLab Series</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey">Image Segmentation Using Deep Learning: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering">PointRend: Image Segmentation as Rendering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need">Transformer: Attention is all you need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks (SENet)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module">CBAM: Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN: A convolutional neural network for low-light image enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO: Vision Outlooker for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[36]SOLO-Segmenting-Objects-by-Locations">SOLO: Segmenting Objects by Locations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT: Real-time Instance Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[41]Generative-Adversarial-Networks">Generative Adversarial Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">[43]RepVGG-Making-VGG-style-ConvNets-Great-Again</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[46]Demystifying-Local-Vision-Transformer">[46]Demystifying-Local-Vision-Transformer</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_eoK2"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_e+kA"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Feature Pyramid Networks for Object Detection</h1></header><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>ã€‚<a aria-hidden="true" class="hash-link" href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust" title="Direct link to heading">â€‹</a></h3><p>åŸè®ºæ–‡<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Networks for Object Detection</a>ã€‚</p><p>è¿™ç¯‡è®ºæ–‡å°±æ˜¯å¤§å®¶ç†ŸçŸ¥çš„FPNäº†ã€‚FPNæ˜¯<strong>æ¯”è¾ƒæ—©æœŸçš„ä¸€ä»½å·¥ä½œ</strong>ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ç¯‡è®ºæ–‡åªæ˜¯å¤šå°ºåº¦ç‰¹å¾èåˆçš„ä¸€ç§æ–¹å¼ã€‚ä¸è¿‡è¿™ç¯‡è®ºæ–‡æå‡ºçš„æ¯”è¾ƒæ—©ï¼ˆCVPR2017ï¼‰ï¼Œåœ¨å½“æ—¶çœ‹æ¥æ˜¯éå¸¸å…ˆè¿›çš„ï¼‰ï¼Œåœ¨å½“æ—¶å…·æœ‰å¾ˆå¤šäº®ç‚¹ï¼šFPNä¸»è¦è§£å†³çš„æ˜¯ç‰©ä½“æ£€æµ‹ä¸­çš„å¤šå°ºåº¦é—®é¢˜ï¼Œé€šè¿‡ç®€å•çš„ç½‘ç»œè¿æ¥æ”¹å˜ï¼Œåœ¨åŸºæœ¬ä¸å¢åŠ åŸæœ‰æ¨¡å‹è®¡ç®—é‡æƒ…å†µä¸‹ï¼Œå¤§å¹…åº¦æå‡äº†å°ç‰©ä½“æ£€æµ‹çš„æ€§èƒ½ã€‚</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstractæ‘˜è¦">Abstractï¼ˆæ‘˜è¦ï¼‰<a aria-hidden="true" class="hash-link" href="#abstractæ‘˜è¦" title="Direct link to heading">â€‹</a></h2><blockquote><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p></blockquote><p>è¿™ç¯‡è®ºæ–‡å¯¹ä»¥åçš„è®¸å¤šç½‘ç»œè®¾è®¡äº§ç”Ÿäº†è¾ƒå¤§çš„å½±å“ï¼Œæ¨èä½ é˜…è¯»<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">åŸæ–‡</a>ã€‚è¿™é‡Œåªæ˜¯å¯¹è¿™ç¯‡è®ºæ–‡çš„ç²—æµ…é˜…è¯»ç¬”è®°ã€‚</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ä»‹ç»introduction">ä»‹ç»ï¼ˆIntroductionï¼‰<a aria-hidden="true" class="hash-link" href="#ä»‹ç»introduction" title="Direct link to heading">â€‹</a></h2><p>è¯¥è®ºæ–‡æå‡ºï¼Œç‰¹å¾é‡‘å­—å¡”æ˜¯è¯†åˆ«ç³»ç»Ÿä¸­ç”¨äºæ£€æµ‹ä¸åŒæ¯”ä¾‹ç‰©ä½“çš„åŸºæœ¬ç»„ä»¶ï¼Œç”šè‡³å·ç§°æ‰‹å·¥ç‰¹å¾è®¾è®¡æ—¶ä»£çš„ä¸‡é‡‘æ²¹ï¼šæ¯”å¦‚åœ¨OpenCVåº“çš„ç‰¹å¾åŒ¹é…Cascadeåˆ†ç±»å™¨ç”¨äºäººè„¸è¯†åˆ«ä¸­ä½¿ç”¨ç‰¹å¾é‡‘å­—å¡”æ¨¡å‹+AdaBoostæå–ä¸åŒå°ºåº¦ç‰¹å¾ç»è¡Œåˆ†ç±»ç­‰ã€‚</p><p>åŸè®ºæ–‡è¿™æ ·å½¢å®¹å¤šå°ºåº¦çš„å¥½å¤„ï¼š</p><blockquote><p>The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p></blockquote><p>åœ¨è¿›å…¥æ·±åº¦å·ç§¯ç½‘ç»œä¹‹åï¼Œå¦‚ä½•åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­æ›´å¥½åœ°åˆ©ç”¨å¤šå°ºåº¦ç§°ä¸ºäº†ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼Œä»…ä½¿ç”¨æ·±åº¦å·ç§¯ç½‘ç»œè¿›è¡Œæ£€æµ‹ä¼šå¯¼è‡´å°ç›®æ ‡çš„æ¼æ£€ï¼›å¦ä¸€æ–¹é¢ï¼Œåœ¨æ¯å±‚ä¸åŒçº§åˆ«çš„ç‰¹å¾å›¾ä¸Šè¿›è¡Œé¢„æµ‹äº§ç”Ÿäº†å¤šä½™çš„æ€§èƒ½æ¶ˆè€—ï¼Œå¹¶ä¸”æ•ˆæœå¹¶æ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆå¥½ã€‚è¿™ç¯‡è®ºæ–‡å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜çš„ã€‚</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ç›¸å…³å·¥ä½œrelated-works">ç›¸å…³å·¥ä½œï¼ˆRelated worksï¼‰<a aria-hidden="true" class="hash-link" href="#ç›¸å…³å·¥ä½œrelated-works" title="Direct link to heading">â€‹</a></h2><ul><li>æ‰‹å·¥è®¾è®¡çš„ç‰¹å¾å’Œæ—©æœŸç¥ç»ç½‘ç»œï¼ˆHand-engineered features and early neural networksï¼‰</li><li>æ™®é€šçš„æ·±åº¦å·ç§¯ç›®æ ‡æ£€æµ‹ç½‘ç»œï¼ˆDeep ConvNet object detectorsï¼‰</li><li>èåˆäº†æ·±åº¦å·ç§¯ç½‘ç»œçš„ç‰¹å¾é‡‘å­—å¡”æ¨¡å‹ï¼ˆMethods using multiple layersï¼‰</li></ul><p><img alt="image-20210512231141905" src="/assets/images/image-20210512231141905-b65472fe00bc89cbbae7520f4333ccf7.png"></p><p>ä¸Šå›¾ä¸ºåŸè®ºæ–‡ä¸­å‡ºç°çš„ç¤ºæ„å›¾ã€‚å…¶ä¸­ï¼š</p><ul><li><p>(a)æ˜¯æ‰‹å·¥è®¾è®¡ç‰¹å¾æè¿°æ—¶ä»£çš„å¸¸è§æ¨¡å‹ï¼Œå³å¯¹ä¸åŒå°ºå¯¸çš„å›¾ç‰‡æå–ç‰¹å¾ï¼Œä»¥æ»¡è¶³ä¸åŒå°ºåº¦ç›®æ ‡çš„æ£€æµ‹è¦æ±‚ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</p></li><li><p>(b)æ˜¯æ™®é€šçš„æ·±åº¦å·ç§¯ç½‘ç»œæ¨¡å¼ï¼Œé€šè¿‡ä¸‹é‡‡æ ·æ‰©å¤§æ„Ÿå—é‡ï¼Œæå–è¯­ä¹‰ä¿¡æ¯ã€‚</p></li><li><p>(c)æ˜¯èåˆäº†æ·±åº¦å·ç§¯ç½‘ç»œçš„ç‰¹å¾é‡‘å­—å¡”æ¨¡å‹ã€‚æ·±åº¦å·ç§¯ç½‘ç»œåœ¨å·ç§¯è¿‡ç¨‹ä¸­æ¯å±‚éƒ½ä¼šäº§ç”Ÿä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œæ‰€ä»¥å…¶æœ¬èº«å°±å¤©ç„¶å…·æœ‰é‡‘å­—å¡”ç»“æ„ã€‚æˆªæ­¢åˆ°è¿™ç¯‡è®ºæ–‡çš„å†™ä½œæ—¶é—´ä¸ºæ­¢ï¼Œæœ‰å¾ˆå¤šåœ¨COCOå’ŒImageNetä¸Šè¡¨ç°ä¼˜ç§€çš„ç½‘ç»œéƒ½æ˜¯pyramid representationsçš„ï¼Œå³è®©ä¸åŒå±‚é¢„æµ‹ä¸åŒå°ºåº¦çš„ç‰©ä½“ã€‚ä½†æ˜¯å…¶å¯¹å°ç›®æ ‡çš„æ£€æµ‹æ•ˆæœä»ç„¶ä¸å¤Ÿå¥½ï¼ŒåŸå› åœ¨äºä½å°ºåº¦çš„ç‰¹å¾å›¾åŒ…å«çš„è¯­ä¹‰ä¿¡æ¯è¿˜ä¸å¤Ÿæ·±åˆ»ï¼ˆè¯´ä¿—è¯å°±æ˜¯æ¬ å·äº†ï¼‰ï¼ŒåŸæ–‡æ˜¯è¿™æ ·è¯´çš„ï¼š</p><blockquote><p>This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p></blockquote><p>SSDå°±æ˜¯ç¬¬ä¸€æ‰¹é‡‡ç”¨è¿™ç§æ–¹æ³•çš„æ·±åº¦å·ç§¯ç½‘ç»œä¹‹ä¸€ã€‚ä¸è¿‡SSDä¸ºäº†é¿å…ä½¿ç”¨è¯­ä¹‰ä¿¡æ¯ä¸è¶³çš„ç‰¹å¾å›¾ï¼ŒSSDå¹¶æ²¡èƒ½å¾ˆå¥½åœ°å¤ç”¨å·²æœ‰çš„ç‰¹å¾å›¾ï¼Œè¿™è®©å®ƒå¯¹å°ç›®æ ‡çš„æ£€æµ‹æ•ˆæœä»ç„¶ä¸å¤Ÿå¥½ã€‚å·ç§¯ç¥ç»ç½‘ç»œçš„æ·±åº¦å¾€å¾€å’Œæ¯ä¸€æ­¥çš„å·ç§¯çš„æ­¥é•¿å‚æ•°æ˜¯ä¸€ä¸ªå¾ˆçŸ›ç›¾çš„ä¸œè¥¿ã€‚å±•å¼€æ¥è¯´ï¼Œç½‘ç»œæ›´æ·±çš„æ—¶å€™ï¼Œå°±ä¸å¾—ä¸é€šè¿‡å°†æ­¥é•¿æ”¹å¤§ä»¥å¹³è¡¡æ›´æ·±çš„ç½‘ç»œå¸¦æ¥çš„å‚æ•°é‡ä¸Šæ¶¨é—®é¢˜ã€‚ä½†åŒæ—¶è¿™å¯¼è‡´äº†å¦å¤–ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯æ­¥é•¿å¾ˆå¤§çš„æ—¶å€™ï¼Œç”šè‡³å¯ä»¥å¤§è¿‡è¾“å…¥å›¾åƒä¸­ä¸€äº›ç‰©ä½“çš„å¤§å°ï¼Œä½¿å¾—ä¸€äº›ç›®æ ‡ä¸¢å¤±ã€‚</p></li><li><p>(d)æ˜¯è¿™ç¯‡è®ºæ–‡è¦æå‡ºçš„FPNç½‘ç»œç»“æ„ã€‚è¯¥ç½‘ç»œåœ¨è®¾è®¡æ—¶çš„ä¸€ä¸ªç›®æ ‡å°±æ˜¯é¿å…(c)ä¸­å‡ºç°çš„é—®é¢˜ï¼Œè®©æ¯ä¸€ä¸ªå°ºåº¦çš„ç‰¹å¾å›¾éƒ½ä¼šåŒ…å«è¶³å¤Ÿä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚</p></li></ul><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œfeature-pyramid-networks">ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFeature Pyramid Networksï¼‰<a aria-hidden="true" class="hash-link" href="#ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œfeature-pyramid-networks" title="Direct link to heading">â€‹</a></h2><p>åŸè®ºæ–‡è¿™æ ·æè¿°è¿™ç¯‡è®ºæ–‡çš„ç›®çš„ï¼š</p><blockquote><p>Our goal is to leverage a ConvNetâ€™s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.</p></blockquote><p>ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ç¯‡è®ºæ–‡çš„ç›®çš„å°±æ˜¯åˆ©ç”¨æ·±åº¦å·ç§¯ç½‘ç»œå¤©ç„¶å­˜åœ¨çš„é‡‘å­—å¡”ç‰¹å¾å±‚æ¬¡ç»“æ„ï¼šè¯¥å±‚æ¬¡ç»“æ„åˆ©ç”¨è‡ªåº•å‘ä¸Šçš„é€å±‚çš„å·ç§¯è·å¾—ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­æ„å»ºå…·æœ‰é«˜å±‚è¯­ä¹‰çš„ç‰¹å¾é‡‘å­—å¡”ã€‚</p><p>è¿™ç¯‡è®ºæ–‡çš„ç½‘ç»œç»“æ„è®¾è®¡ä¸»è¦åŒ…å«äº†ä¸¤ä¸ªéƒ¨åˆ†ï¼š</p><ul><li>è‡ªåº•å‘ä¸Šï¼ˆBottom-up pathwayï¼‰</li><li>è‡ªé¡¶å‘ä¸‹ï¼ˆTop-down pathway and lateral connectionsï¼‰</li></ul><p>åœ¨æ¥ä¸‹æ¥çš„è¯´æ˜ä¸­ï¼Œä¼šç»å¸¸ä½¿ç”¨<code>stage</code>çš„æ¦‚å¿µã€‚åœ¨è¿™é‡Œè¿›è¡Œæå‰å®šä¹‰è¯´æ˜ï¼šåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œç½‘ç»œä¸­è¾“å‡ºçš„feature mapå¤§å°ç›¸åŒçš„å±‚è¢«ç§°ä¸ºæ˜¯åŒä¸ªstageçš„ã€‚ä¸åŒstageä¼šäº§ç”Ÿä¸åŒå¤§å°çš„ç‰¹å¾å›¾ï¼Œè¿™ç¯‡è®ºæ–‡ä¸ºæ¯ä¸ªstageå®šä¹‰ä¸€ä¸ªé‡‘å­—å¡”å±‚çº§ï¼ˆpyramid levelï¼‰ã€‚</p><p><img alt="image-20210513105442041" src="/assets/images/image-20210513105442041-bf83a73de8877a088888077f30e8e37f.png"></p><p>ä¾‹å¦‚ä¸Šå›¾æ˜¯VGG-16çš„ç½‘ç»œç»“æ„å›¾ï¼Œå›¾ä¸­ç”¨ç´«è‰²çš„æ–‡å­—æ ‡å‡ºäº†5ä¸ªä¸åŒçš„stageã€‚å½“ç„¶ï¼Œä¸æŒ¨åœ¨ä¸€èµ·ä½†æ˜¯è¾“å‡ºç‰¹å¾å›¾å¤§å°ç›¸åŒçš„å±‚ä¹Ÿæ˜¯å±äºåŒä¸€ä¸ªstageçš„ã€‚</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è‡ªåº•å‘ä¸Šbottom-up-pathway">è‡ªåº•å‘ä¸Šï¼ˆBottom-up pathwayï¼‰<a aria-hidden="true" class="hash-link" href="#è‡ªåº•å‘ä¸Šbottom-up-pathway" title="Direct link to heading">â€‹</a></h3><p>è‡ªåº•å‘ä¸Šå°±æ˜¯æ™®é€šæ·±åº¦å·ç§¯ç½‘ç»œå‰å‘ä¼ æ’­çš„è¿‡ç¨‹ã€‚å¯¹äºæåŠçš„é‡‘å­—å¡”ï¼ˆpyramidï¼‰ç‰¹å¾ï¼Œè¿™ç¯‡è®ºæ–‡é€‰æ‹©æ¯ä¸ªstageæœ€åä¸€å±‚çš„è¾“å‡ºä½œä¸ºä¸€ä¸ªç‰¹å¾å›¾ï¼Œè¿™ä¸ªé€‰æ‹©å¬ä¸Šå»å¾ˆåˆç†ï¼Œå› ä¸ºè¶Šä¸Šå±‚ï¼ˆæ¯ä¸ªstageçš„æœ€åä¸€ä¸ªå±‚ï¼‰çš„ç‰¹å¾å°±åŒ…å«æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚</p><p><img alt="image-20210513105825996" src="/assets/images/image-20210513105825996-5e0eb11494ad89300caf9a419b3dba24.png"></p><p>ä¾‹å¦‚ï¼Œåœ¨ä¸Šå›¾ä¸­ï¼Œè¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•é€‰æ‹©å³ä¾§çš„ç‰¹å¾å›¾è¾“å‡ºä½œä¸ºä¸€ä¸ªstageçš„ç‰¹å¾å›¾è¾“å‡ºè€Œä¸æ˜¯å·¦ä¾§çš„ã€‚</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="è‡ªé¡¶å‘ä¸‹top-down-pathway-and-lateral-connections">è‡ªé¡¶å‘ä¸‹ï¼ˆTop-down pathway and lateral connectionsï¼‰<a aria-hidden="true" class="hash-link" href="#è‡ªé¡¶å‘ä¸‹top-down-pathway-and-lateral-connections" title="Direct link to heading">â€‹</a></h3><p>è‡ªé¡¶å‘ä¸‹çš„ç»“æ„åœ¨ç›´è§‚æ„Ÿå—ä¸Šæ˜¯å’Œæ·±åº¦å·ç§¯ç½‘ç»œä¸‹é‡‡æ ·çš„è¿‡ç¨‹ç›¸åçš„ï¼šå®ƒå°†é«˜å±‚stageè¾“å‡ºçš„ç©ºé—´ä¿¡æ¯å¾ˆç²—åŠ£ï¼ˆæ¯•ç«Ÿä¹‹å‰ä¸€ç›´åœ¨ä¸‹é‡‡æ ·...ï¼‰ä½†æ˜¯è¯­ä¹‰ä¿¡æ¯å¾ˆä¸°å¯Œçš„ç‰¹å¾å›¾è¿›è¡Œä¸æ–­ä¸Šé‡‡æ ·ï¼Œä¸Šé‡‡æ ·åˆ°å’Œå…¶å¯¹åº”çš„stageçš„ä¸Šä¸€ä¸ªstageçš„ç‰¹å¾å›¾å¤§å°ï¼Œå†é€šè¿‡å›¾ä¸­è¿™ç§ä¾§è¾¹çš„è¿æ¥ï¼ˆlateral connectionsï¼‰ï¼Œä½¿ç”¨ä½ä¸€çº§stageçš„ç‰¹å¾å›¾æ¥å¢å¼ºå®ƒã€‚</p><p><img alt="image-20210513110511093" src="/assets/images/image-20210513110511093-4117637496c0968d7839c6776dd6b66e.png"></p><p>ä¸Šå›¾æ˜¯è®ºæ–‡ä¸­ä¾§è¾¹è¿æ¥ï¼ˆlateral connectionsï¼‰çš„æ–¹å¼ã€‚æ¯ä¸ªä¾§è¾¹è¿æ¥å°†åŒä¸ªstageçš„è‡ªåº•å‘ä¸Šçš„ç‰¹å¾å›¾å’Œè‡ªé¡¶å‘ä¸‹çš„ç‰¹å¾å›¾ç›¸åŠ ã€‚æ³¨æ„ï¼Œè¯¥è®ºæ–‡ä¸­ç‰¹æŒ‡äº†æ¯ä¸¤ä¸ªstageä¹‹é—´çš„ç©ºé—´åˆ†è¾¨ç‡ä¹‹å·®æ˜¯2å€ï¼Œä¹Ÿå°±æ˜¯ä¸‹é‡‡æ ·ç‡æ˜¯2å€ã€‚æ‰€ä»¥ä¸Šå›¾çš„ç¤ºæ„ä¸­é è¿‘é¡¶å±‚çš„ç‰¹å¾å›¾ç»è¿‡2å€ä¸Šé‡‡æ ·åä½¿ç”¨ä¾§è¾¹è¿æ¥å’Œä¸Šä¸€çº§ç‰¹å¾å›¾ç›¸åŠ ç»„æˆå¢å¼ºåçš„æ–°ç‰¹å¾ï¼›æŠ€æœ¯ä¸Šï¼Œåœ¨ä¾§è¾¹è¿æ¥çš„è¿‡ç¨‹ä¸­ï¼Œè¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•ä½¿ç”¨<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>Ã—</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>å·ç§¯çš„æ–¹å¼ä½¿éœ€è¦ç›¸åŠ çš„ç‰¹å¾å›¾é€šé“æ•°ä¿æŒä¸€è‡´ã€‚</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="åº”ç”¨applications">åº”ç”¨ï¼ˆApplicationsï¼‰<a aria-hidden="true" class="hash-link" href="#åº”ç”¨applications" title="Direct link to heading">â€‹</a></h2><p>æœ‰ç©ºå°±å†™ã€‚</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/unlimited-paper-works/[08]Dynamic-Neural-Networks-A-Survey"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« <!-- -->Dynamic Neural Networks: A Survey</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A Review on Deep Learning Techniques Applied to Semantic Segmentation<!-- --> Â»</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯visualdust" class="table-of-contents__link toc-highlight">è¿™ç¯‡ç¬”è®°çš„å†™ä½œè€…æ˜¯VisualDustã€‚</a></li><li><a href="#abstractæ‘˜è¦" class="table-of-contents__link toc-highlight">Abstractï¼ˆæ‘˜è¦ï¼‰</a></li><li><a href="#ä»‹ç»introduction" class="table-of-contents__link toc-highlight">ä»‹ç»ï¼ˆIntroductionï¼‰</a></li><li><a href="#ç›¸å…³å·¥ä½œrelated-works" class="table-of-contents__link toc-highlight">ç›¸å…³å·¥ä½œï¼ˆRelated worksï¼‰</a></li><li><a href="#ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œfeature-pyramid-networks" class="table-of-contents__link toc-highlight">ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFeature Pyramid Networksï¼‰</a><ul><li><a href="#è‡ªåº•å‘ä¸Šbottom-up-pathway" class="table-of-contents__link toc-highlight">è‡ªåº•å‘ä¸Šï¼ˆBottom-up pathwayï¼‰</a></li><li><a href="#è‡ªé¡¶å‘ä¸‹top-down-pathway-and-lateral-connections" class="table-of-contents__link toc-highlight">è‡ªé¡¶å‘ä¸‹ï¼ˆTop-down pathway and lateral connectionsï¼‰</a></li></ul></li><li><a href="#åº”ç”¨applications" class="table-of-contents__link toc-highlight">åº”ç”¨ï¼ˆApplicationsï¼‰</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e23c63c0.js"></script>
<script src="/assets/js/main.f6709b37.js"></script>
</body>
</html>