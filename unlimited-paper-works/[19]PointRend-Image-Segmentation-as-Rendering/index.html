<!doctype html>
<html class="docs-version-current" lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">PointRend: Image Segmentation as Rendering | 工具箱的深度学习记事簿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-docs-current"><meta data-react-helmet="true" property="og:title" content="PointRend: Image Segmentation as Rendering | 工具箱的深度学习记事簿"><meta data-react-helmet="true" name="description" content="image-20210601121147760"><meta data-react-helmet="true" property="og:description" content="image-20210601121147760"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.54fd31ab.css">
<link rel="preload" href="/assets/js/runtime~main.e23c63c0.js" as="script">
<link rel="preload" href="/assets/js/main.f6709b37.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">工具箱的深度学习记事簿</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/unlimited-paper-works/">魔法部日志</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">工具箱的深度学习记事簿</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menuLinkText_OKON">魔法部日志（又名论文阅读日志）</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/">欢迎来到魔法部日志</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[01]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[02]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[03]Progressive-Semantic-Segmentation">Progressive Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[04]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[05]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[06]DeepLab-Series">DeepLab Series</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[07]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[08]Dynamic-Neural-Networks-A-Survey">Dynamic Neural Networks: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[09]Feature-Pyramid-Networks-for-Object-Detection">Feature Pyramid Networks for Object Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey">Image Segmentation Using Deep Learning: A Survey</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition">Gated Channel Transformation for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module">Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri-Image-Segmentation-Evaluation">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering">PointRend: Image Segmentation as Rendering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need">Transformer: Attention is all you need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks">Squeeze-and-Excitation Networks (SENet)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module">CBAM: Convolutional Block Attention Module</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[27]Non-local-Neural-Networks">Non-local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks">Disentangled Non-Local Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement">LLCNN: A convolutional neural network for low-light image enhancement</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[33]VOLO-Vision-Outlooker-for-Visual-Recognition">VOLO: Vision Outlooker for Visual Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[34]Polarized-Self-Attention-Towards-High-quality-Pixel-wise-Regression">Polarized Self-Attention: Towards High-quality Pixel-wise Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[35]SimAM-A-Simple-Parameter-Free-Attention-Module-for-Convolutional-Neural-Networks">SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[36]SOLO-Segmenting-Objects-by-Locations">SOLO: Segmenting Objects by Locations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[37]YOLACT-Real-time-Instance-Segmentation">YOLACT: Real-time Instance Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[38]You-Only-Look-One-level-Feature">You Only Look One-level Feature</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[39]Instance-sensitive-Fully-Convolutional-Networks">Instance-sensitive Fully Convolutional Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[40]Learning-in-the-Frequency-Domain">Learning in the Frequency Domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[41]Generative-Adversarial-Networks">Generative Adversarial Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation">[42]CCNet-Criss-Cross-Attention-for-Semantic-Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[43]RepVGG-Making-VGG-style-ConvNets-Great-Again">[43]RepVGG-Making-VGG-style-ConvNets-Great-Again</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network">[44]PP-LCNet-A-Lightweight-CPU-Convolutional-Neural-Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows">[45]Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/unlimited-paper-works/[46]Demystifying-Local-Vision-Transformer">[46]Demystifying-Local-Vision-Transformer</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_eoK2"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_e+kA"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>PointRend: Image Segmentation as Rendering</h1></header><p><img alt="image-20210601121147760" src="/assets/images/image-20210601121147760-2609d0f40692dce369b8041f749ff63b.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="这篇笔记的写作者是visualdust">这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust</a>。<a aria-hidden="true" class="hash-link" href="#这篇笔记的写作者是visualdust" title="Direct link to heading">​</a></h3><blockquote><p>“我们希望预测分割图的边界区域更加准确，我们就不应该使用均匀采样，而应该更加倾向于图像边界区域。”</p></blockquote><p>这是一篇用于改善图像分割问题中边缘分割效果的方法的论文的阅读笔记。该方法“将分割问题看作渲染问题”，达到了较好的效果。论文原文：<a href="https://arxiv.org/abs/1912.08193" target="_blank" rel="noopener noreferrer">PointRend: Image Segmentation as Rendering</a>。在阅读这篇笔记之前，请确保先了解图像分割技术。对分割的技术进行简要的了解，可以参考<a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.md">另一篇笔记</a>。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="abstract摘要">Abstract（摘要）<a aria-hidden="true" class="hash-link" href="#abstract摘要" title="Direct link to heading">​</a></h2><blockquote><p>We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend&#x27;s efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend" target="_blank" rel="noopener noreferrer">this https URL</a>.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_y2LR" id="介绍introduction">介绍（Introduction）<a aria-hidden="true" class="hash-link" href="#介绍introduction" title="Direct link to heading">​</a></h2><p>我们希望预测分割图的边界区域更加准确，我们就不应该使用均匀采样，而应该更加倾向于图像边界区域。这种类似的采样问题在计算机图形学中已经被研究了几十年了，图像渲染，将一个模型（比如，3D网格）映射为一个rasterized image（即一个像素的规则网格），虽然输出是规则的，但是计算的时候却不是根据网格上均匀采样来计算。常见的策略就是在图像平面内自适应地采样一些点产生不规则的子集，再来进行计算。</p><p>这篇文章的中心思想就是将图像分割问题视作图像渲染问题，使用来自于计算机图像学中的经典思想设计到神经网络中，渲染出更高质量的分割图。这个设计的核心内容是 PointRend（基于点的渲染）神经网络模块：“一个基于迭代细分算法在自适应选择的位置执行基于点的分割预测的模块”。该模块使用细分策略自适应地选择一组非均匀点来计算标签，容易理解的说法是该模块自动选取边缘上难以正确归类的点进行再次分类。 它的细分策略使用比直接密集计算少一个数量级的浮点运算来有效地计算高分辨率分割图。</p><p>请注意，PointRend是一个网络模块而不是独立的网络。该模块接受一个或多个CNN输出的feature map，并产生比输入更高分辨率的预测。</p><blockquote><p>PointRend is a general module that admits many possible implementations.</p></blockquote><p>PointRend 可以合并到流行的元架构中，用于实例分割（例如，Mask R-CNN）和语义分割（例如FCN）。</p><p><img alt="image-20210601154033700" src="/assets/images/image-20210601154033700-ceeafefadcb8e011f2ceaf99eae03342.png"></p><p>上图是一种可能的PointRend结构模式图：选择一些分割困难的点，CNN产生的特征图被输入到MLP中，和粗分割结果融合，最终输出分割困难点的预测结果。</p><p>实际上，这篇论文的创新在于重新选择边缘困难点的种类。本质上这篇论文其实是一个新型上采样方法，针对物体边缘的图像分割进行优化，使其在难以分割的物体边缘部分有更好的表现。在整个PointRend设计的过程中有类似于渲染的思想，但请不要对“渲染”过度理解。</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="方法method">方法（Method）<a aria-hidden="true" class="hash-link" href="#方法method" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pointrend模块">PointRend模块<a aria-hidden="true" class="hash-link" href="#pointrend模块" title="Direct link to heading">​</a></h3><p>类似于在计算机图形学中，屏幕上某个位置的像素是从一个曲线、模型等通过被称为渲染的映射产生，在计算机视觉中，我们可以将图像分割视为底层连续实体的类别图，而分割输出，即预测标签的像素集合，是从中“渲染”出来的。</p><p><img alt="image-20210601161409751" src="/assets/images/image-20210601161409751-e24699b1b93174f9709af1dd459cf743.png"></p><p>上图是这个模式的简图。其中，PointRend的关键步骤就是训练一个“解码器”（或者叫“渲染器”），从channel中“解码”（或者“渲染”）出预测困难的像素所属的类别。</p><p>PointRend模块接受一个或多个典型的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span>通道的CNN特征图（<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">f\in \R^{C\times H\times W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span></span></span></span></span></span></span></span></span></span></span></span></span>）作为输入，这些输入特征图往往比需要预测的图像的实际尺寸要小4~16倍。PointRend模块会输出一个大小为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>k</mi><mo>×</mo><msup><mi>H</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">p\in \R^{k\times H&#x27; \times W&#x27;}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9425em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9425em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>的对K个类别的预测。通常输出的大小会大于输入的大小。</p><p>（如果读不懂了请先参考<a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.md">另一篇笔记</a>了解图像的分割技术）</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="pointrend模块的组成">PointRend模块的组成<a aria-hidden="true" class="hash-link" href="#pointrend模块的组成" title="Direct link to heading">​</a></h3><p>PointRend模块由单个主要的部分组成：</p><ol><li>一个<strong>点的选择策略</strong>。PointRend模块并不会对整幅图片上的所有点进行预测（这样会产生巨大的开销），而是选择其中的一部分看上去“难以预测的实值点”进行预测（实值点的特征是通过输入<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span></span>的双线性插值计算的）。</li><li>一个<strong>特征提取器</strong>（或者叫“解码器”，或是“渲染器”）。对于每个选定的点，在输入中相关的部分可能是<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>×</mo><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">i\in \R^{C\times 1\times 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6986em;vertical-align:-0.0391em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="mbin mtight">×</span><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>的一个长长的通道。对于每个选定的点，特征提取器提取该点的特征表示。</li><li>一个<strong>分类器</strong>（原文中称之为“point head”）。这个分类器是一个很小的神经网络，它被训练来从这个逐点特征表示中预测一个标签。对于这部分来说，每个点都是独立的。</li></ol><p>这几个组成部分将会在下文中进行详细介绍。</p><p>PointRend 架构可应用于实例分割（例如，在 Mask R-CNN上）和语义分割（例如，在 FCNs上）任务。</p><p><img alt="image-20210601154033700" src="/assets/images/image-20210601154033700-ceeafefadcb8e011f2ceaf99eae03342.png"></p><p>如上图，PointRend 模块应用于每个区域。它通过对一组选定点进行预测，以从粗到细的方式计算掩码。对于语义分割，可以将整个图像视为单个区域，因此不失一般性。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="点的选择策略point-selection-for-inference-and-training">点的选择策略（Point selection for Inference and Training）<a aria-hidden="true" class="hash-link" href="#点的选择策略point-selection-for-inference-and-training" title="Direct link to heading">​</a></h3><p>PointRend方法的核心是灵活和自适应地选择图像平面中预测分割标签的点。直觉上，这些点应该更密集地靠近高频区域，例如物体边界，类似于光线追踪中的抗锯齿问题。</p><p>PointRend的推理选择策略受到计算机图形学中自适应细分的经典技术的启发。该技术用于通过仅在值与其邻居显著不同的位置进行计算来高效地渲染高分辨率图像。对于其他位置，这些值是通过插入已经计算出的输出值获得的。下面是一段解释：</p><p><img alt="image-20210601170513086" src="/assets/images/image-20210601170513086-cd985d9c3a302c63476e10af1c75d8b8.png"></p><p>在渲染问题中，例如，在一块分辨率为1080p的屏幕上显示一张2k的图片时，往往显示设备不需要对2k图片的所有像素都进行处理，仅处理一部分即可显示出足够的分辨率。但当用户放大图片，在1080p的显示屏上显示一张2k图片中的一角时，显示设备就需要对这张图片进行更精细的处理让它们出现在屏幕上显示更多细节。通过观察我们发现，1和2部分和原来的像素差距不大，不需要从图片文件渲染，处理时仅需同上文中的“这些值是通过插入已经计算出的输出值获得的”，从放大前的图上插值即可得到。需要重新处理的，是1和2的边缘部分，也就是上面提到的“值与其邻居显著不同的位置”。</p><blockquote><p> 类比渲染问题中的细分过程和分割问题中的上采样过程后我们发现，在分割中的上采样过程中时我们需要关注边缘，对边缘进行细化即可。</p></blockquote><p>所以点的选择策略，就是在上采样过程中选择“不确定的点”，或“边界的点”。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="前向传播过程inference">前向传播过程（Inference）<a aria-hidden="true" class="hash-link" href="#前向传播过程inference" title="Direct link to heading">​</a></h4><blockquote><p>对于每个区域，我们以粗到细的方式迭代地“渲染”输出掩码。</p></blockquote><p>在网络的前向传播过程到达PointRend模块时，会总体经历以下步骤：</p><ol><li>使用双线性插值对其先前预测的分割进行上采样。</li><li>在这个上采样的特征图上选择 N 个最不确定的点（例如，对于二进制掩码，可以选择概率最接近 0.5 的那些点）。</li><li>为这 N 个点中的每一个计算逐点特征表示（在第 3.2 节中简要描述）并预测它们的标签。</li><li>重复这个过程，直到分割被上采样到所需的分辨率。</li></ol><p><img alt="image-20210601200340192" src="/assets/images/image-20210601200340192-59cd2a8f5cd1b6534484ba977e6f0586.png"></p><p>上图是这个过程的放在整个网络中的大致流程示意图，下面是这个过程的局部流程图。</p><p><img alt="image-20210601205123801" src="/assets/images/image-20210601205123801-73471884e7139efaee43a8da0ec7f24d.png"></p><p>整个过程就是比双线性插值的上采样多了一个选择不确定点和预测其种类的过程。整个前向传播的过程简单易懂。</p><p>输入大小为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mn>0</mn></msub><mo>×</mo><msub><mi>M</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">M_0 \times M_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>并且输出大小为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span>的PointRend模块一次运算所需要预测的像素总量不会超过<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mfrac><mi>M</mi><msub><mi>M</mi><mn>0</mn></msub></mfrac></mrow><annotation encoding="application/x-tex">N \log_2 {\frac{M}{M_0}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3174em;vertical-align:-0.4451em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.109em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span>，这比<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span>小得多，并使PointRend 更有效地进行高分辨率预测。例如，如果<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">M_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>为 7 并且所需分辨率为<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">M =224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">224</span></span></span></span></span> ，则执行 5 个细分步骤。如果我们在每一步选择<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn><msup><mn>8</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N =28^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">2</span><span class="mord"><span class="mord">8</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>个点，PointRend 只对$ 28^2 · 4.25 <span class="math math-inline"><span class="katex-error" title="ParseError: KaTeX parse error: Can&#x27;t use function &#x27;$&#x27; in math mode at position 10: 个点进行预测，这比$̲224^2" style="color:#cc0000"><span class="cjk_fallback">个点进行预测，这比$224^2</span></span></span>小 15 倍。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="训练时的不确定点选取training">训练时的“不确定”点选取（Training）<a aria-hidden="true" class="hash-link" href="#训练时的不确定点选取training" title="Direct link to heading">​</a></h4><p>在之前的一些步骤中，我们提到PointRend模块中需要选择一些“不确定”的点进行分类。那么，怎么选择“不确定”的点呢？我们在上面提到:</p><blockquote><p>在这个上采样的特征图上选择 N 个最不确定的点</p></blockquote><p><img alt="image-20210601154033700" src="/assets/images/image-20210601154033700-1622553357332-ceeafefadcb8e011f2ceaf99eae03342.png"></p><p>采样策略在特征图上选择N个点，这N个点应该是“不确定”的。PointRend在选择点时使用三个原则使选择到的点是“不确定”的：</p><ol><li><p>过度生成：平均从输入中选择<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">kN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span>个点（其中<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k&gt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>）。</p></li><li><p>重要性采样：根据粗预测（上方图片中的“coarse prediction”）结果判断每个点的不确定性，从中选择“不确定粗略预测的点”，从刚才选取的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">kN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span>的点中选出最不确定的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\beta N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10903em">βN</span></span></span></span></span>个点（其中<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\beta \in [0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>）。</p></li><li><p>Coverage （说实话暂时没看懂这一个是干啥的）：剩余的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">(1-\beta)N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span>个点是从均匀分布中采样。如下图中采用了不同的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>和<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>来说明这个过程。左侧是均匀分布的示意图，右侧是采用不同的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>和<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>的示意图。</p><p><img alt="image-20210601211914002" src="/assets/images/image-20210601211914002-1f18f04bfd91051515cc655e1c3c7410.png"></p></li></ol><p><code>原则上，这里的选择点的策略可以与上文中推理时点的细分策略类似。**然而，细分的策略会引入对神经网络的反向传播算法不太友好的运算步骤**，所以在选择点的时候这篇论文中选择了非迭代的随机采样策略。</code></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="点的特征提取和分类point-wise-representation-and-point-head">点的特征提取和分类（Point-wise Representation and Point Head）<a aria-hidden="true" class="hash-link" href="#点的特征提取和分类point-wise-representation-and-point-head" title="Direct link to heading">​</a></h3><p>在特征提取部分，PointRend通过融合粗特征和细粒度特征获得更合适的特征，然后通过分类得到该点的类别。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="细粒度特征fine-grained-features">细粒度特征（Fine-grained features）<a aria-hidden="true" class="hash-link" href="#细粒度特征fine-grained-features" title="Direct link to heading">​</a></h4><blockquote><p>细粒度特征就是各种分割网络中用于稠密预测的直接特征。</p></blockquote><p>为了让 PointRend 呈现精细的分割细节，Point Rend方法中从前序CNN输出的特征图中的每个像素位置提取一个特征向量，然后进行双线性插值上采样作为这个位置像素的细粒度特征。可以从单个特征映射中提取这个特征（例如ResNet中的<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><msub><mi>s</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">res_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord mathnormal">re</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>）；或者，也可以从多个特征图融合得到（例如<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><msub><mi>s</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">res_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord mathnormal">re</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>到<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><msub><mi>s</mi><mn>5</mn></msub></mrow><annotation encoding="application/x-tex">res_5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord mathnormal">re</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>，或是从它们的特征金字塔对应层的输出提取）。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="粗预测特征coarse-prediction-features">粗预测特征（Coarse prediction features）<a aria-hidden="true" class="hash-link" href="#粗预测特征coarse-prediction-features" title="Direct link to heading">​</a></h4><blockquote><p>粗粒度让特征“更具空间性”。</p></blockquote><p>单纯使用细粒度特征容易导致先天性的不足，那就是和周围较大范围内的像素很难产生关联，也就是所谓的空间信息丢失。这是前序CNN下采样导致的结果。例如，同时处在两个不同实体边界上的点只具有一份相同的细粒度特征，但是一个点只能被分给一个实体（也换句话说就是具有相同细粒度特征的点在不同的实体区域内应该被分类为不同标签），这就需要该点的额外的区域信息。</p><p>还有，根据产生细粒度特征所使用的特征图的不同，可能会出现细粒度特征内只包含相对低级的信息的情况。在这种情况下，具有更多上下文和语义信息的特征源会对分割的精度产生很大的帮助。</p><p>基于这些考虑，第二种特征类型是来自网络的粗分割预测，即区域（框）中每个点的 K 维向量代表 K 类预测。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="分类器点头point-head">分类器（“点头”，Point Head）<a aria-hidden="true" class="hash-link" href="#分类器点头point-head" title="Direct link to heading">​</a></h4><p>点头。给定每个选定点的逐点特征表示，PointRend 使用简单的多层感知器 (MLP) 进行逐点分割预测。这个 MLP 在所有点（和所有区域）上共享权重，类似于图卷积或 PointNet。由于 MLP 预测每个点的分割标签，它可以通过标准的特定于任务的分割损失进行训练。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="实验experiments">实验（Experiments）<a aria-hidden="true" class="hash-link" href="#实验experiments" title="Direct link to heading">​</a></h2><p>请参考原文。</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« <!-- -->Involution: Inverting the Inherence of Convolution for Visual Recognition</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Transformer: Attention is all you need<!-- --> »</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#这篇笔记的写作者是visualdust" class="table-of-contents__link toc-highlight">这篇笔记的写作者是VisualDust。</a></li><li><a href="#abstract摘要" class="table-of-contents__link toc-highlight">Abstract（摘要）</a></li><li><a href="#介绍introduction" class="table-of-contents__link toc-highlight">介绍（Introduction）</a></li><li><a href="#方法method" class="table-of-contents__link toc-highlight">方法（Method）</a><ul><li><a href="#pointrend模块" class="table-of-contents__link toc-highlight">PointRend模块</a></li><li><a href="#pointrend模块的组成" class="table-of-contents__link toc-highlight">PointRend模块的组成</a></li><li><a href="#点的选择策略point-selection-for-inference-and-training" class="table-of-contents__link toc-highlight">点的选择策略（Point selection for Inference and Training）</a></li><li><a href="#点的特征提取和分类point-wise-representation-and-point-head" class="table-of-contents__link toc-highlight">点的特征提取和分类（Point-wise Representation and Point Head）</a></li></ul></li><li><a href="#实验experiments" class="table-of-contents__link toc-highlight">实验（Experiments）</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e23c63c0.js"></script>
<script src="/assets/js/main.f6709b37.js"></script>
</body>
</html>