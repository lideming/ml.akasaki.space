<!doctype html>
<html class="docs-version-current" lang="zh-cn" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><title data-react-helmet="true">图像语义分割综述 | 工具箱的深度学习记事簿</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://ml.akasaki.space//ch3p1/[3]overview-of-semantic-segmentation"><meta data-react-helmet="true" name="docusaurus_locale" content="zh-cn"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-docs-current"><meta data-react-helmet="true" property="og:title" content="图像语义分割综述 | 工具箱的深度学习记事簿"><meta data-react-helmet="true" name="description" content="这是一篇关于综述论文的解读。原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）"><meta data-react-helmet="true" property="og:description" content="这是一篇关于综述论文的解读。原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）"><link data-react-helmet="true" rel="shortcut icon" href="/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://ml.akasaki.space//ch3p1/[3]overview-of-semantic-segmentation"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//ch3p1/[3]overview-of-semantic-segmentation" hreflang="zh-cn"><link data-react-helmet="true" rel="alternate" href="https://ml.akasaki.space//ch3p1/[3]overview-of-semantic-segmentation" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.54fd31ab.css">
<link rel="preload" href="/assets/js/runtime~main.e23c63c0.js" as="script">
<link rel="preload" href="/assets/js/main.f6709b37.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/img/logo.svg" alt="Logo" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">工具箱的深度学习记事簿</b></a><a class="navbar__item navbar__link" href="/unlimited-paper-works/">魔法部日志</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/neet-cv/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">🌜</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">🌞</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_lDyR"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_i9tI" type="button"></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">首页</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">第零章：在开始之前</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">第一章上：HelloWorld</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">第一章下：深度学习基础</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">第二章上：卷积神经网络</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">第二章下：经典卷积神经网络</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#">第三章上：谈一些计算机视觉方向</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ch3p1/[1]deep-learning-for-computer-vision">深度学习之于计算机视觉</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ch3p1/[2]overview-of-target-detection">目标检测论文综述</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ch3p1/[3]overview-of-semantic-segmentation">图像语义分割综述</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ch3p1/[4]image-style-transfer">风格迁移</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">第三章下：了解更高级的技术</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">附录1：好朋友们</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">附录2：数学是真正的圣经</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">附录3：信号和采样的学问（DSP）</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">附录4：TensorFlow编程策略</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/unlimited-paper-works/">魔法部日志（又名论文阅读日志）</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_eoK2"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_e+kA"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L theme-doc-toc-mobile tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>图像语义分割综述</h1></header><p>这是一篇关于综述论文的解读。<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）</a></p><p>摘要：</p><blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote><p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">原作</a>。本文只是对原作阅读的粗浅笔记。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="介绍分割">介绍分割<a aria-hidden="true" class="hash-link" href="#介绍分割" title="Direct link to heading">​</a></h2><p>对图像进行分割主要有：语义分割（Semantic segmentation）和实例分割（Instance segmentation）。它们的区别一目了然：</p><p><img alt="image-20210427154733807" src="/assets/images/image-20210427154733807-39f0c9b4ad2e6cca317e8b6faf075822.png"></p><p>左图：原图；中图：语义分割；右图：实例分割。</p><p>很明显，语义分割希望将不同类别的物体所在位置的像素分开来，但是对于相同类别的不同物体并不敏感；而实例分割不但需要分开每一个位置上像素属于哪一类，还要分出它具体属于哪一个对象。</p><p>我们知道一个图像只不过是许多像素的集合。图像分割分类是对图像中属于特定类别的像素进行分类的过程，因此<strong>图像分割可以认为是按像素进行分类的问题</strong>。</p><p>如果你对离散数学以及softmax很敏感的化，肯定第一时间会产生这样的联想：</p><p><img alt="image-20210427222245438" src="/assets/images/image-20210427222245438-149ef48133be5926deb58514f203e85c.png"></p><p>这张图实际上是这样的：</p><p><img alt="image-20210427222340602" src="/assets/images/image-20210427222340602-84b084faa6a175bd4c5468bbc0261321.png"></p><p>当然，对于实际应用中通道数量的具体数字可根据实际需求选择。例如，在前景分割中，仅需分割出前景和背景，因此只需要一个通道。而全景分割中，如果使用类one-hot编码，则需要有和对象数目+1一样多的通道数。</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="分割的技术">分割的技术<a aria-hidden="true" class="hash-link" href="#分割的技术" title="Direct link to heading">​</a></h2><p>在深度学习方法流行之前，TextonForest和基于随机森林分类器等语义分割方法是用得比较多的方法。但是本文章的背景是基于深度学习方法的计算机视觉，所以不做过多讨论。</p><p>深度学习技术在各个计算机领域获得了巨大的成功，其解决语义分割问题可以概括为几种思路：</p><ul><li>块分类（Patch classification）</li><li>全卷积方法（基于FCN）</li><li>编码器-解码器结构（encoder-decoder，本质基于FCN）</li><li>跨层连接的encoder-decoder结构</li></ul><h3 class="anchor anchorWithStickyNavbar_y2LR" id="块分类patch-classification">块分类（Patch classification）<a aria-hidden="true" class="hash-link" href="#块分类patch-classification" title="Direct link to heading">​</a></h3><p>块分类算得上是一类最古老的方法。</p><p>如其名，把图像分成小块塞给网络进行分类。分成指定大小的小块是因为全连接网络只接受指定大小的输入。这大概是最初的基于深度学习的分割方法了（吧）。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="全卷积方法基于fcn">全卷积方法（基于FCN）<a aria-hidden="true" class="hash-link" href="#全卷积方法基于fcn" title="Direct link to heading">​</a></h3><p>全卷积方法在块分类之后，优势是使用全卷积代替了块分类中的全连接。</p><p>用于代替全连接的全卷积方法除了在其他视觉方法里很出名，也很快用到了分割算法中。2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。（我用简单分类模型实测了一下也是，全连接真的是太烂了，又慢又重，但是作为多层感知机到全卷积网路中间的过度组件，还是功不可没的。）</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="插值法实现的上采样">插值法实现的上采样<a aria-hidden="true" class="hash-link" href="#插值法实现的上采样" title="Direct link to heading">​</a></h4><p>在全卷积方法中，为了使输出和输入大小相同，在卷积导致特征图变小后还需要经过上采样使特征图变为原来大小。</p><p><img alt="deconv01" src="/assets/images/deconv01-12cbc555dbcc715b8e007679f890b701.gif"></p><p>上图：一种反卷积的示意。其中蓝色较小的特征图是输入，通过在它周围填充，使其变为较大的特征图后，再进行卷积。得到的结果是绿色的特征图。</p><p><img alt="deconv02" src="/assets/images/deconv02-e9c47ef7664faf59e94f6cf0bb2916f0.gif"></p><p>上图：另一种反卷积的示意。其中蓝色较小的特征图经过某种填充方法进行填充，变为较大的特征图后再进行卷积。</p><p>反卷积的常见思路是通过一些填充的方法将较小的特征图变大，然后通过卷积获得比原来的小特征图更大的特征图。较为常用的填充方法是插值法。</p><p>插值的方法主要可以分为两类，一类是线性图像插值方法：</p><ul><li>最近邻插值(Nearest neighbor interpolation)</li><li>双线性插值(Bi-Linear interpolation)</li><li>双立方插值(Bi-Cubic interpolation)</li></ul><p>另一类是非线性图像插值方法：</p><ul><li>基于小波变换的插值算法</li><li>基于边缘信息的插值算法。</li></ul><p>以上的这些方法都是一些插值方法，需要我们在决定网络结构的时候进行挑选。这些方法就像是人工特征工程一样，并没有给神经网络学习的余地，神经网络不能自己学习如何更好地进行插值，这个显然是不够理想的。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="转置卷积实现的上采样">转置卷积实现的上采样<a aria-hidden="true" class="hash-link" href="#转置卷积实现的上采样" title="Direct link to heading">​</a></h4><p>在上采样的方法中，比较出名的是转置卷积，因为它允许我们使用可学习的上采样过程。</p><p>典型的转置卷积运算将采用滤波器视图中当前值的点积并作为相应的输出位置产生的单个值，而转置卷积的过程基本想法。对于转置卷积，我们从低分辨率特征图中获取单个值，并将滤波器中的所有权重乘以该值，将加权值输出到更大的特征图。</p><p><img alt="image-20210427223356560" src="/assets/images/image-20210427223356560-eda2e0fa6f2471e9fb7ae85483e99f65.png"></p><p>上图：转置卷积的一种示意。</p><blockquote><p>Tips：神经网络中的解卷积层也被称作：转置卷积(Transposed Convolution)、上卷积（upconvolution）、完全卷积（full convolution）、转置卷积（transposed convolution）、微步卷积（fractionally-strided convolution）。</p><p>转置卷积常常在一些文献中也称之为反卷积(Deconvolution)和部分跨越卷积(Fractionally-strided Convolution)，因为称之为反卷积容易让人以为和数字信号处理中反卷积混起来，造成不必要的误解，因此下文都将称为转置卷积，并且建议各位不要采用反卷积这个称呼。</p></blockquote><h3 class="anchor anchorWithStickyNavbar_y2LR" id="编码器-解码器结构encoder-decoder本质基于fcn">编码器-解码器结构（encoder-decoder，本质基于FCN）<a aria-hidden="true" class="hash-link" href="#编码器-解码器结构encoder-decoder本质基于fcn" title="Direct link to heading">​</a></h3><p>encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。</p><p><img alt="image-20210428220457279" src="/assets/images/image-20210428220457279-a78a6a13eebb96f40ea9a69600a225d0.png"></p><p>实际上，符合下采样提取特征，再上采样恢复原大小的都可以称为encoder-decoder结构。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="跨层连接的encoder-decoder结构">跨层连接的encoder-decoder结构<a aria-hidden="true" class="hash-link" href="#跨层连接的encoder-decoder结构" title="Direct link to heading">​</a></h4><p>通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接，其思想我猜是从VGG跨层连接出现的思想）。</p><p><img alt="image-20210427221642324" src="/assets/images/image-20210427221642324-a345eb6ead9c3b337830854dd9f673fb.png"></p><p>上图是带有跨层连接的encoder-decoder的代表之一：UNet的结构。</p><h4 class="anchor anchorWithStickyNavbar_y2LR" id="高低层特征融合">高低层特征融合<a aria-hidden="true" class="hash-link" href="#高低层特征融合" title="Direct link to heading">​</a></h4><p>由于池化操作造成的信息损失，上采样（即使采用解卷积操作）只能生成粗略的分割结果图。因此，论文从高分辨率的特征图中引入跳跃连接（shortcut/skip connection）操作改善上采样的精细程度（感觉像是从ResNet开始出现的思想）：</p><p><img alt="FCN-2" src="/assets/images/FCN-2-2ea09f8a6d42611290674a123976c76e.png"></p><p>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。可以看到如上三行的对应的结果：</p><p><img alt="FCN-3" src="/assets/images/FCN-3-ab6307d70d4e66cf08c57096a9485edc.png"></p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="空洞卷积dilatedatrous-convolution代替了池化-上采样的过程">空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）<a aria-hidden="true" class="hash-link" href="#空洞卷积dilatedatrous-convolution代替了池化-上采样的过程" title="Direct link to heading">​</a></h3><p>尽管FCN及encoder-decoder结构中移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作。这里使用池化的下采样为例：pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p><p>Dilated/Atrous Convolution（空洞卷积），这种结构代替了池化，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息（我觉得这个设计很有意思，原图的大小完全不会改变，也不需要上采样了）。</p><p><img alt="image-20210427221923919" src="/assets/images/image-20210427221923919-66e60b92bfcda45838fac56515297bfd.png"></p><p>上图：在某篇论文中出现的空洞卷积示意图。</p><p><img alt="Atrous_conv" src="/assets/images/Atrous_conv-8bbc9b569825f67962e113f732b01b61.png"></p><p>上图：另一张空洞卷积的示意图。</p><h3 class="anchor anchorWithStickyNavbar_y2LR" id="条件随机场">条件随机场<a aria-hidden="true" class="hash-link" href="#条件随机场" title="Direct link to heading">​</a></h3><p>在使用全卷积网络的分割方法中，有一个很常用的基本框架：</p><p><img alt="img" src="/assets/images/CRF01-f823011d924cce31a8c37c26aff7d561.jpg"></p><p>其中， FCN 表示各种全卷积网络，CRF 为条件随机场，MRF 为马尔科夫随机场。其大致思路就是前端使用 FCN 进行特征粗提取，后端使用 CRF/MRF 优化前端的输出，最后得到分割图。</p><p><a href="https://arxiv.org/pdf/1210.5644.pdf" target="_blank" rel="noopener noreferrer">条件随机场（Conditional Random Field，CRF）</a> 后处理操作通常用于进一步改善分割的效果。CRFs 是一种基于底层图像的像素强度进行“平滑”分割（‘smooth’ segmentation）的图模型，其工作原理是相似强度的像素更可能标记为同一类别。CRFs 一般能够提升 1-2% 的精度。</p><p><img alt="CRF" src="/assets/images/CRF-624c9c654b86b81285c6a4f7a2c1b892.png"></p><p>上图为CRF示意图。（b）一元分类结合CRF;（c, d, e）是CRF的变体，其中(e)是广泛使用的一种CRF。</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="分割的数据集">分割的数据集<a aria-hidden="true" class="hash-link" href="#分割的数据集" title="Direct link to heading">​</a></h2><p>截止到原综述写作时间为止时较为流行的数据集：</p><p><img alt="image-20210428094548476" src="/assets/images/image-20210428094548476-a29569648f65f36a58b06b15f8f24bc9.png"></p><p>还没看完，看完就写。</p><hr><h2 class="anchor anchorWithStickyNavbar_y2LR" id="领域知名论文">领域知名论文<a aria-hidden="true" class="hash-link" href="#领域知名论文" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_y2LR" id="基于深度学习的分割方法">基于深度学习的分割方法<a aria-hidden="true" class="hash-link" href="#基于深度学习的分割方法" title="Direct link to heading">​</a></h3><p><img alt="image-20210428094705161" src="/assets/images/image-20210428094705161-bfe0531f2b6fbf9c4e2820660c89f60b.png"></p><ol><li><p>FCN</p><p>主要贡献：使端对端的卷积语义分割网络变得流行起来；通过deconvolutional layers进行上采样；通过skip connection改善了上采样的粗糙度。</p></li><li><p>SegNet</p><p>主要贡献：使用Maxpooling indices来增强位置信息。</p></li><li><p>Dilated Convolutions</p><p>主要贡献：使用空洞卷积用来进行稠密预测（dense prediction）；提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。</p></li><li><p>DeepLab (v1 &amp; v2)</p><p>主要贡献：使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率；提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息；使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。</p></li><li><p>RefineNet</p><p>主要贡献：精心设计了encoder-decoder架构中的decoder部分，使得性能提升；整个网络的设计都遵循residual connections，网络表达能力更强，梯度更容易反向传播。</p></li><li><p>PSPNet</p><p>主要贡献：使用pyramid pooling整合context；使用auxiliary loss。</p></li><li><p>Large Kernel Matters</p><p>主要贡献：提出一种具有非常大的内核卷积的编码器-解码器体系结构。</p></li><li><p>DeepLab v3</p><p>主要贡献：改进的无孔空间金字塔池化（ASPP）；级联使用atrous卷积的模块。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_y2LR" id="上述方法的关系">上述方法的关系<a aria-hidden="true" class="hash-link" href="#上述方法的关系" title="Direct link to heading">​</a></h3><p><img alt="image-20210428094839526" src="/assets/images/image-20210428094839526-9350dccfcbd7ce8dc703d2d73ace40cd.png"></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/ch3p1/[2]overview-of-target-detection"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« <!-- -->目标检测论文综述</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/ch3p1/[4]image-style-transfer"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">风格迁移<!-- --> »</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#介绍分割" class="table-of-contents__link toc-highlight">介绍分割</a></li><li><a href="#分割的技术" class="table-of-contents__link toc-highlight">分割的技术</a><ul><li><a href="#块分类patch-classification" class="table-of-contents__link toc-highlight">块分类（Patch classification）</a></li><li><a href="#全卷积方法基于fcn" class="table-of-contents__link toc-highlight">全卷积方法（基于FCN）</a></li><li><a href="#编码器-解码器结构encoder-decoder本质基于fcn" class="table-of-contents__link toc-highlight">编码器-解码器结构（encoder-decoder，本质基于FCN）</a></li><li><a href="#空洞卷积dilatedatrous-convolution代替了池化-上采样的过程" class="table-of-contents__link toc-highlight">空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）</a></li><li><a href="#条件随机场" class="table-of-contents__link toc-highlight">条件随机场</a></li></ul></li><li><a href="#分割的数据集" class="table-of-contents__link toc-highlight">分割的数据集</a></li><li><a href="#领域知名论文" class="table-of-contents__link toc-highlight">领域知名论文</a><ul><li><a href="#基于深度学习的分割方法" class="table-of-contents__link toc-highlight">基于深度学习的分割方法</a></li><li><a href="#上述方法的关系" class="table-of-contents__link toc-highlight">上述方法的关系</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 neet-cv. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e23c63c0.js"></script>
<script src="/assets/js/main.f6709b37.js"></script>
</body>
</html>